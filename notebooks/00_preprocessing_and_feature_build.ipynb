{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric Validation of AI-Generated 3D Wheel Meshes Using Engineering Constraints\n",
    "\n",
    "## Preprocessing and Feature Engineering Notebook\n",
    "\n",
    "**Author:** Mahil Kattilparambath Ramakrishnan  \n",
    "**Course:** QM 640 Data Analytics Capstone, Walsh College  \n",
    "**Date:** February 2026  \n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook builds the complete engineered feature dataset from DeepWheel simulation results and 3D mesh files (STL). The goal is to compute parametric features across 7 engineering validation layers (Layer 0-6) that serve as quality gates for AI-generated wheel designs.\n",
    "\n",
    "---\n",
    "\n",
    "## The 7 Engineering Validation Layers\n",
    "\n",
    "| Layer | Name | Description |\n",
    "|-------|------|-------------|\n",
    "| 0 | Data Integrity & Scale | Bounding box checks, unit scale validation |\n",
    "| 1 | Mesh Validity | Geometric soundness: watertight, manifold, triangle quality |\n",
    "| 2 | Feature Extractability | Confidence indicators for CAD feature extraction |\n",
    "| 3 | Physics Plausibility | Modal analysis outputs, frequency ratios, stiffness proxies |\n",
    "| 4 | Engineering Constraints | Pass/fail evaluation against design thresholds |\n",
    "| 5 | Manufacturability | Heuristic indicators for CAD/manufacturing readiness |\n",
    "| 6 | Analytics & ML Readiness | Normalized features, risk scoring, classification labels |\n",
    "\n",
    "Each layer produces a **gate output** (`layerX_pass` or `layerX_status`) and a **failure reason string**.\n",
    "\n",
    "---\n",
    "\n",
    "## Important Disclaimers\n",
    "\n",
    "1. **Manufacturability metrics are heuristic indicators**, not certified manufacturing approval. They provide directional guidance but do not replace formal DFM (Design for Manufacturing) analysis.\n",
    "\n",
    "2. **Layer 2 feature extractability scores are confidence measures**, not guarantees of successful CAD feature recognition. They indicate the likelihood that automated tools can extract features, not that they will.\n",
    "\n",
    "3. **Physics plausibility uses DeepWheel simulation outputs**. These are computational approximations and should be validated with physical testing for production use.\n",
    "\n",
    "4. All thresholds are **conservative and documented**. They are based on typical automotive wheel specifications and can be adjusted for specific applications.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- `data/processed/deepwheel_features_full.csv` - Complete feature dataset\n",
    "- `data/processed/deepwheel_data_dictionary.csv` - Column definitions\n",
    "- `data/processed/quality_report.json` - Summary statistics and failure analysis\n",
    "- `docs/figures/Figure1_dataset_preview.png` - First 10 rows preview\n",
    "- `docs/figures/Figure2_folder_tree_structure.png` - Repository structure\n",
    "- `docs/figures/Figure3_3d_wheel_preview.png` - Sample wheel renderings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trimesh version: 4.11.1\n",
      "tqdm available\n",
      "\n",
      "============================================================\n",
      "Environment Ready\n",
      "Python: 3.11.14 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 18:30:03) [MSC v.1929 64 bit (AMD64)]\n",
      "Pandas: 2.3.3\n",
      "NumPy: 1.26.4\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 2: Imports and Environment Checks\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Check and install trimesh if missing\n",
    "try:\n",
    "    import trimesh\n",
    "    print(f\"trimesh version: {trimesh.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Installing trimesh...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"trimesh\", \"-q\"])\n",
    "    import trimesh\n",
    "    print(f\"trimesh installed: {trimesh.__version__}\")\n",
    "\n",
    "# Check and install tqdm if missing\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    print(\"tqdm available\")\n",
    "except ImportError:\n",
    "    print(\"Installing tqdm...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tqdm\", \"-q\"])\n",
    "    from tqdm import tqdm\n",
    "    print(\"tqdm installed\")\n",
    "\n",
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Environment Ready\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Structure:\n",
      "  Project Root: C:\\Users\\mahil.kr\\GL\\data-analytics-capstone\n",
      "  Data Directory: C:\\Users\\mahil.kr\\GL\\data-analytics-capstone\\data\n",
      "  STL Directory: C:\\Users\\mahil.kr\\GL\\data-analytics-capstone\\data\\stl (exists: True)\n",
      "  STEP Directory: C:\\Users\\mahil.kr\\GL\\data-analytics-capstone\\data\\step (exists: True)\n",
      "  Processed Output: C:\\Users\\mahil.kr\\GL\\data-analytics-capstone\\data\\processed (created: True)\n",
      "  Figures Output: C:\\Users\\mahil.kr\\GL\\data-analytics-capstone\\docs\\figures (created: True)\n",
      "\n",
      "Simulation CSV: C:\\Users\\mahil.kr\\GL\\data-analytics-capstone\\data\\deepwheel_sim_results.csv (exists: True)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 3: Define Paths and Create Output Folders\n",
    "# =============================================================================\n",
    "\n",
    "# Define base paths (notebook is in notebooks/ folder)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "if NOTEBOOK_DIR.name == 'notebooks':\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR\n",
    "\n",
    "# Input paths\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "SIM_RESULTS_CSV = DATA_DIR / 'deepwheel_sim_results.csv'\n",
    "STL_DIR = DATA_DIR / 'stl'\n",
    "STEP_DIR = DATA_DIR / 'step'\n",
    "\n",
    "# Output paths\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "FIGURES_DIR = PROJECT_ROOT / 'docs' / 'figures'\n",
    "\n",
    "# Output files\n",
    "OUTPUT_FEATURES_CSV = PROCESSED_DIR / 'deepwheel_features_full.csv'\n",
    "OUTPUT_DATA_DICT_CSV = PROCESSED_DIR / 'deepwheel_data_dictionary.csv'\n",
    "OUTPUT_QUALITY_JSON = PROCESSED_DIR / 'quality_report.json'\n",
    "\n",
    "# Figure outputs\n",
    "FIG1_PREVIEW = FIGURES_DIR / 'Figure1_dataset_preview.png'\n",
    "FIG2_TREE = FIGURES_DIR / 'Figure2_folder_tree_structure.png'\n",
    "FIG3_WHEEL = FIGURES_DIR / 'Figure3_3d_wheel_preview.png'\n",
    "\n",
    "# Create output directories\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project Structure:\")\n",
    "print(f\"  Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"  Data Directory: {DATA_DIR}\")\n",
    "print(f\"  STL Directory: {STL_DIR} (exists: {STL_DIR.exists()})\")\n",
    "print(f\"  STEP Directory: {STEP_DIR} (exists: {STEP_DIR.exists()})\")\n",
    "print(f\"  Processed Output: {PROCESSED_DIR} (created: {PROCESSED_DIR.exists()})\")\n",
    "print(f\"  Figures Output: {FIGURES_DIR} (created: {FIGURES_DIR.exists()})\")\n",
    "print(f\"\\nSimulation CSV: {SIM_RESULTS_CSV} (exists: {SIM_RESULTS_CSV.exists()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading simulation results...\n",
      "Original columns: ['file_name', 'Mass', 'Mode7 Freq', 'Mode11 Freq']\n",
      "\n",
      "Dataset loaded successfully:\n",
      "  Total designs: 904\n",
      "  Columns: ['file_name', 'mass', 'mode7_freq', 'mode11_freq', 'mass_original', 'mode7_freq_original', 'mode11_freq_original']\n",
      "\n",
      "Basic statistics:\n",
      "             mass  mode7_freq  mode11_freq\n",
      "count  904.000000  904.000000   904.000000\n",
      "mean    20.514222  417.427495  1134.729746\n",
      "std      1.142835   21.299897   113.825731\n",
      "min     17.615000  366.075400   910.437200\n",
      "25%     19.701300  402.080750  1052.335250\n",
      "50%     20.527300  417.026000  1142.719500\n",
      "75%     21.290100  432.166000  1215.616000\n",
      "max     24.146400  479.600300  1512.935000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 4: Load Base Simulation CSV\n",
    "# =============================================================================\n",
    "\n",
    "# Load the simulation results\n",
    "print(\"Loading simulation results...\")\n",
    "df_raw = pd.read_csv(SIM_RESULTS_CSV)\n",
    "\n",
    "# Store original column names\n",
    "original_columns = df_raw.columns.tolist()\n",
    "print(f\"Original columns: {original_columns}\")\n",
    "\n",
    "# Create normalized column name mapping\n",
    "def normalize_column_name(col):\n",
    "    \"\"\"Convert column name to lowercase with underscores.\"\"\"\n",
    "    return col.lower().replace(' ', '_').replace('-', '_')\n",
    "\n",
    "# Create the main dataframe with normalized names\n",
    "df = df_raw.copy()\n",
    "column_mapping = {col: normalize_column_name(col) for col in df.columns}\n",
    "df = df.rename(columns=column_mapping)\n",
    "\n",
    "# Verify required columns exist\n",
    "required_columns = ['file_name', 'mass', 'mode7_freq', 'mode11_freq']\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\\n\"\n",
    "                     f\"Available columns: {df.columns.tolist()}\\n\"\n",
    "                     f\"Please ensure the CSV contains: file_name, Mass, Mode7 Freq, Mode11 Freq\")\n",
    "\n",
    "# Keep original column names as separate columns for reference\n",
    "df['mass_original'] = df['mass']\n",
    "df['mode7_freq_original'] = df['mode7_freq']\n",
    "df['mode11_freq_original'] = df['mode11_freq']\n",
    "\n",
    "print(f\"\\nDataset loaded successfully:\")\n",
    "print(f\"  Total designs: {len(df)}\")\n",
    "print(f\"  Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(df[['mass', 'mode7_freq', 'mode11_freq']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping file names to STL/STEP paths...\n",
      "\n",
      "File availability summary:\n",
      "  Total designs in CSV: 904\n",
      "  Designs with STL: 904 (100.0%)\n",
      "  Designs with STEP: 904 (100.0%)\n",
      "  Designs missing STL: 0\n",
      "  Designs missing STEP: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 5: Map file_name to STL/STEP Paths\n",
    "# =============================================================================\n",
    "\n",
    "def find_stl_path(file_name, stl_dir):\n",
    "    \"\"\"Find STL file path, trying common naming patterns.\"\"\"\n",
    "    # Primary pattern: direct match\n",
    "    primary = stl_dir / f\"{file_name}.stl\"\n",
    "    if primary.exists():\n",
    "        return str(primary)\n",
    "    \n",
    "    # Try lowercase\n",
    "    lower = stl_dir / f\"{file_name.lower()}.stl\"\n",
    "    if lower.exists():\n",
    "        return str(lower)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def find_step_path(file_name, step_dir):\n",
    "    \"\"\"Find STEP file path, trying .stp and .step extensions.\"\"\"\n",
    "    # Try .stp first (more common in this dataset)\n",
    "    stp_path = step_dir / f\"{file_name}.stp\"\n",
    "    if stp_path.exists():\n",
    "        return str(stp_path)\n",
    "    \n",
    "    # Try .step\n",
    "    step_path = step_dir / f\"{file_name}.step\"\n",
    "    if step_path.exists():\n",
    "        return str(step_path)\n",
    "    \n",
    "    # Try lowercase\n",
    "    stp_lower = step_dir / f\"{file_name.lower()}.stp\"\n",
    "    if stp_lower.exists():\n",
    "        return str(stp_lower)\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"Mapping file names to STL/STEP paths...\")\n",
    "\n",
    "# Map paths\n",
    "df['stl_path'] = df['file_name'].apply(lambda x: find_stl_path(x, STL_DIR))\n",
    "df['step_path'] = df['file_name'].apply(lambda x: find_step_path(x, STEP_DIR))\n",
    "\n",
    "# Create boolean flags\n",
    "df['has_stl'] = df['stl_path'].notna()\n",
    "df['has_step'] = df['step_path'].notna()\n",
    "\n",
    "# Summary\n",
    "stl_count = df['has_stl'].sum()\n",
    "step_count = df['has_step'].sum()\n",
    "total = len(df)\n",
    "\n",
    "print(f\"\\nFile availability summary:\")\n",
    "print(f\"  Total designs in CSV: {total}\")\n",
    "print(f\"  Designs with STL: {stl_count} ({100*stl_count/total:.1f}%)\")\n",
    "print(f\"  Designs with STEP: {step_count} ({100*step_count/total:.1f}%)\")\n",
    "print(f\"  Designs missing STL: {total - stl_count}\")\n",
    "print(f\"  Designs missing STEP: {total - step_count}\")\n",
    "\n",
    "# Show sample of missing files\n",
    "missing_stl = df[~df['has_stl']]['file_name'].head(5).tolist()\n",
    "if missing_stl:\n",
    "    print(f\"\\nSample designs missing STL: {missing_stl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Layer 0: Data Integrity & Scale Consistency\n",
      "============================================================\n",
      "Scale thresholds: 100mm - 600mm per axis\n",
      "\n",
      "Processing 904 STL files for bounding box...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 0 - Bounding Box: 100%|████████████████████████████████████████████████████████| 904/904 [01:27<00:00, 10.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer 0 Summary:\n",
      "  Passed: 904 (100.0%)\n",
      "  Failed: 0\n",
      "\n",
      "Bounding box statistics (mm):\n",
      "           bbox_x      bbox_y      bbox_z\n",
      "count  904.000000  904.000000  904.000000\n",
      "mean   482.600000  482.600000  215.900000\n",
      "std      0.000008    0.000008    0.000004\n",
      "min    482.599991  482.599991  215.899994\n",
      "25%    482.599991  482.599991  215.899994\n",
      "50%    482.600006  482.600006  215.900002\n",
      "75%    482.600006  482.600006  215.900002\n",
      "max    482.600006  482.600006  215.900009\n",
      "\n",
      "Axis orientation check:\n",
      "  Correctly oriented (Z is smallest): 904\n",
      "  Incorrectly oriented: 0\n",
      "  Heuristic: Z-axis should be wheel width (smallest bbox dimension)\n",
      "  X and Y should be wheel diameter (larger, roughly equal)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 6: Layer 0 - Data Integrity & Scale Consistency\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Computing Layer 0: Data Integrity & Scale Consistency\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define thresholds for typical automotive wheel dimensions (in mm)\n",
    "# Typical wheel diameter: 14-22 inches (355-559mm)\n",
    "# Typical wheel width: 5-12 inches (127-305mm)\n",
    "# Adding margin for bounding box (which includes spokes extending to rim)\n",
    "BBOX_MIN_MM = 100  # Minimum dimension in mm\n",
    "BBOX_MAX_MM = 600  # Maximum dimension in mm\n",
    "\n",
    "print(f\"Scale thresholds: {BBOX_MIN_MM}mm - {BBOX_MAX_MM}mm per axis\")\n",
    "\n",
    "# Initialize Layer 0 columns\n",
    "df['bbox_x'] = np.nan\n",
    "df['bbox_y'] = np.nan\n",
    "df['bbox_z'] = np.nan\n",
    "df['bbox_volume_proxy'] = np.nan\n",
    "df['unit_scale_flag'] = np.nan\n",
    "df['axis_orientation_flag'] = np.nan  # Not computed - documented below\n",
    "df['layer0_pass'] = False\n",
    "df['layer0_fail_reason'] = ''\n",
    "\n",
    "# Process each design with STL\n",
    "stl_indices = df[df['has_stl']].index\n",
    "print(f\"\\nProcessing {len(stl_indices)} STL files for bounding box...\")\n",
    "\n",
    "for idx in tqdm(stl_indices, desc=\"Layer 0 - Bounding Box\"):\n",
    "    stl_path = df.loc[idx, 'stl_path']\n",
    "    fail_reasons = []\n",
    "    \n",
    "    try:\n",
    "        # Load mesh\n",
    "        mesh = trimesh.load(stl_path)\n",
    "        \n",
    "        # Get bounding box dimensions\n",
    "        bounds = mesh.bounds  # [[min_x, min_y, min_z], [max_x, max_y, max_z]]\n",
    "        bbox = bounds[1] - bounds[0]  # [size_x, size_y, size_z]\n",
    "        \n",
    "        df.loc[idx, 'bbox_x'] = bbox[0]\n",
    "        df.loc[idx, 'bbox_y'] = bbox[1]\n",
    "        df.loc[idx, 'bbox_z'] = bbox[2]\n",
    "        df.loc[idx, 'bbox_volume_proxy'] = bbox[0] * bbox[1] * bbox[2]\n",
    "        \n",
    "        # Check scale consistency\n",
    "        # Assume dimensions are in mm (typical for CAD exports)\n",
    "        scale_ok = all(BBOX_MIN_MM <= dim <= BBOX_MAX_MM for dim in bbox)\n",
    "        df.loc[idx, 'unit_scale_flag'] = not scale_ok\n",
    "        \n",
    "        if not scale_ok:\n",
    "            too_small = [f\"{['x','y','z'][i]}={bbox[i]:.1f}\" for i, dim in enumerate(bbox) if dim < BBOX_MIN_MM]\n",
    "            too_large = [f\"{['x','y','z'][i]}={bbox[i]:.1f}\" for i, dim in enumerate(bbox) if dim > BBOX_MAX_MM]\n",
    "            if too_small:\n",
    "                fail_reasons.append(f\"bbox_too_small({','.join(too_small)})\")\n",
    "            if too_large:\n",
    "                fail_reasons.append(f\"bbox_too_large({','.join(too_large)})\")\n",
    "        \n",
    "        # Axis orientation check: Z-axis should be the smallest dimension (wheel width)\n",
    "        # For a properly oriented wheel: X ≈ Y (diameter) > Z (width)\n",
    "        min_dim_axis = np.argmin(bbox)  # 0=X, 1=Y, 2=Z\n",
    "        orientation_ok = (min_dim_axis == 2)  # Z should be smallest (axle direction)\n",
    "        df.loc[idx, 'axis_orientation_flag'] = not orientation_ok  # True = problem\n",
    "        \n",
    "        if not orientation_ok:\n",
    "            axis_names = ['X', 'Y', 'Z']\n",
    "            fail_reasons.append(f\"wrong_axis_orientation(smallest={axis_names[min_dim_axis]})\")\n",
    "        \n",
    "        # Gate logic: pass if scale OK AND orientation OK\n",
    "        layer0_pass = scale_ok and orientation_ok\n",
    "        df.loc[idx, 'layer0_pass'] = layer0_pass\n",
    "        df.loc[idx, 'layer0_fail_reason'] = '; '.join(fail_reasons) if fail_reasons else ''\n",
    "        \n",
    "    except Exception as e:\n",
    "        df.loc[idx, 'layer0_pass'] = False\n",
    "        df.loc[idx, 'layer0_fail_reason'] = f\"mesh_load_error: {str(e)[:50]}\"\n",
    "\n",
    "# Handle designs without STL\n",
    "no_stl_mask = ~df['has_stl']\n",
    "df.loc[no_stl_mask, 'layer0_pass'] = False\n",
    "df.loc[no_stl_mask, 'layer0_fail_reason'] = 'no_stl_file'\n",
    "\n",
    "# Summary\n",
    "layer0_pass_count = df['layer0_pass'].sum()\n",
    "print(f\"\\nLayer 0 Summary:\")\n",
    "print(f\"  Passed: {layer0_pass_count} ({100*layer0_pass_count/len(df):.1f}%)\")\n",
    "print(f\"  Failed: {len(df) - layer0_pass_count}\")\n",
    "print(f\"\\nBounding box statistics (mm):\")\n",
    "print(df[['bbox_x', 'bbox_y', 'bbox_z']].describe())\n",
    "\n",
    "# Summary of axis orientation\n",
    "orientation_issues = df['axis_orientation_flag'].sum()\n",
    "print(f\"\\nAxis orientation check:\")\n",
    "print(f\"  Correctly oriented (Z is smallest): {len(df) - orientation_issues}\")\n",
    "print(f\"  Incorrectly oriented: {int(orientation_issues)}\")\n",
    "print(\"  Heuristic: Z-axis should be wheel width (smallest bbox dimension)\")\n",
    "print(\"  X and Y should be wheel diameter (larger, roughly equal)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Layer 1: Mesh Validity (Geometric Soundness)\n",
      "============================================================\n",
      "Using percentile-based thresholds (adaptive to AI-generated mesh quality):\n",
      "  Aspect ratio: top 75% (highest = worst) will be flagged\n",
      "  Watertight: informational only (many AI meshes are not watertight)\n",
      "  Non-manifold edges: informational only\n",
      "\n",
      "Processing 904 STL files for mesh validity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 1 - Mesh Validity: 100%|███████████████████████████████████████████████████████| 904/904 [07:47<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating percentile-based thresholds from data...\n",
      "  Aspect ratio threshold (P75): 39.3\n",
      "  Watertight rate: 5.5%\n",
      "\n",
      "Applying gate logic...\n",
      "\n",
      "Layer 1 Summary:\n",
      "  Passed: 50 (5.5%)\n",
      "  Failed: 854\n",
      "\n",
      "Mesh statistics:\n",
      "  Watertight meshes: 50\n",
      "  Mean triangle count: 91681\n",
      "  Mean edge length: 5.30\n",
      "\n",
      "Note: self_intersection_flag uses broken_faces() as a proxy.\n",
      "Full self-intersection detection requires more sophisticated algorithms.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 7: Layer 1 - Mesh Validity (Geometric Soundness)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Computing Layer 1: Mesh Validity (Geometric Soundness)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Layer 1 uses percentile-based thresholds (adaptive to dataset)\n",
    "# Since AI-generated meshes often have quality issues, we use soft thresholds\n",
    "FAIL_PERCENTILE_L1 = 25  # Bottom 25% on quality metrics will fail\n",
    "\n",
    "print(f\"Using percentile-based thresholds (adaptive to AI-generated mesh quality):\")\n",
    "print(f\"  Aspect ratio: top {100-FAIL_PERCENTILE_L1}% (highest = worst) will be flagged\")\n",
    "print(f\"  Watertight: informational only (many AI meshes are not watertight)\")\n",
    "print(f\"  Non-manifold edges: informational only\")\n",
    "\n",
    "# Initialize Layer 1 columns\n",
    "df['triangle_count'] = np.nan\n",
    "df['is_watertight'] = np.nan\n",
    "df['non_manifold_edge_count'] = np.nan\n",
    "df['self_intersection_flag'] = np.nan  # Limited support in trimesh\n",
    "df['normal_consistency_flag'] = np.nan\n",
    "df['edge_length_mean'] = np.nan\n",
    "df['edge_length_std'] = np.nan\n",
    "df['triangle_aspect_ratio_max'] = np.nan\n",
    "df['layer1_pass'] = False\n",
    "df['layer1_fail_reason'] = ''\n",
    "\n",
    "def compute_triangle_aspect_ratios(mesh):\n",
    "    \"\"\"Compute aspect ratio for each triangle (longest edge / shortest edge).\"\"\"\n",
    "    vertices = mesh.vertices\n",
    "    faces = mesh.faces\n",
    "    \n",
    "    # Get edge lengths for each triangle\n",
    "    v0 = vertices[faces[:, 0]]\n",
    "    v1 = vertices[faces[:, 1]]\n",
    "    v2 = vertices[faces[:, 2]]\n",
    "    \n",
    "    edge1 = np.linalg.norm(v1 - v0, axis=1)\n",
    "    edge2 = np.linalg.norm(v2 - v1, axis=1)\n",
    "    edge3 = np.linalg.norm(v0 - v2, axis=1)\n",
    "    \n",
    "    edges = np.column_stack([edge1, edge2, edge3])\n",
    "    min_edges = np.min(edges, axis=1)\n",
    "    max_edges = np.max(edges, axis=1)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    min_edges = np.maximum(min_edges, 1e-10)\n",
    "    aspect_ratios = max_edges / min_edges\n",
    "    \n",
    "    return aspect_ratios\n",
    "\n",
    "def count_non_manifold_edges(mesh):\n",
    "    \"\"\"Count edges that are not shared by exactly 2 faces.\"\"\"\n",
    "    try:\n",
    "        # Get face adjacency - edges shared by != 2 faces are non-manifold\n",
    "        edges = mesh.edges_unique\n",
    "        edge_faces = mesh.edges_unique_inverse\n",
    "        \n",
    "        # Count faces per edge using edge_to_faces mapping\n",
    "        edge_face_count = np.bincount(edge_faces, minlength=len(edges))\n",
    "        \n",
    "        # Non-manifold edges are those shared by != 2 faces\n",
    "        non_manifold = np.sum((edge_face_count != 2) & (edge_face_count > 0))\n",
    "        return int(non_manifold)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Process each design with STL\n",
    "stl_indices = df[df['has_stl']].index\n",
    "print(f\"\\nProcessing {len(stl_indices)} STL files for mesh validity...\")\n",
    "\n",
    "for idx in tqdm(stl_indices, desc=\"Layer 1 - Mesh Validity\"):\n",
    "    stl_path = df.loc[idx, 'stl_path']\n",
    "    fail_reasons = []\n",
    "    \n",
    "    try:\n",
    "        # Load mesh\n",
    "        mesh = trimesh.load(stl_path)\n",
    "        \n",
    "        # Triangle count\n",
    "        df.loc[idx, 'triangle_count'] = len(mesh.faces)\n",
    "        \n",
    "        # Watertight check\n",
    "        is_watertight = mesh.is_watertight\n",
    "        df.loc[idx, 'is_watertight'] = is_watertight\n",
    "        if not is_watertight:\n",
    "            fail_reasons.append('not_watertight')\n",
    "        \n",
    "        # Non-manifold edges\n",
    "        non_manifold_count = count_non_manifold_edges(mesh)\n",
    "        df.loc[idx, 'non_manifold_edge_count'] = non_manifold_count\n",
    "        if non_manifold_count is not np.nan and non_manifold_count > 0:\n",
    "            fail_reasons.append(f'non_manifold_edges({non_manifold_count})')\n",
    "        \n",
    "        # Self-intersection: Limited in trimesh, use broken_faces as proxy\n",
    "        try:\n",
    "            # Check for degenerate faces as a proxy\n",
    "            degenerate = trimesh.repair.broken_faces(mesh)\n",
    "            df.loc[idx, 'self_intersection_flag'] = len(degenerate) > 0\n",
    "        except:\n",
    "            df.loc[idx, 'self_intersection_flag'] = np.nan\n",
    "        \n",
    "        # Normal consistency (winding consistency)\n",
    "        try:\n",
    "            df.loc[idx, 'normal_consistency_flag'] = mesh.is_winding_consistent\n",
    "        except:\n",
    "            df.loc[idx, 'normal_consistency_flag'] = np.nan\n",
    "        \n",
    "        # Edge lengths\n",
    "        edge_lengths = mesh.edges_unique_length\n",
    "        df.loc[idx, 'edge_length_mean'] = np.mean(edge_lengths)\n",
    "        df.loc[idx, 'edge_length_std'] = np.std(edge_lengths)\n",
    "        \n",
    "        # Triangle aspect ratio\n",
    "        aspect_ratios = compute_triangle_aspect_ratios(mesh)\n",
    "        max_aspect = np.max(aspect_ratios)\n",
    "        df.loc[idx, 'triangle_aspect_ratio_max'] = max_aspect\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Mark as error - will be handled in gate logic below\n",
    "        pass\n",
    "\n",
    "# Handle designs without STL - set metrics to NaN\n",
    "no_stl_mask = ~df['has_stl']\n",
    "\n",
    "# Calculate percentile-based thresholds from computed metrics\n",
    "print(\"\\nCalculating percentile-based thresholds from data...\")\n",
    "ASPECT_RATIO_THRESHOLD = df['triangle_aspect_ratio_max'].quantile((100 - FAIL_PERCENTILE_L1) / 100)\n",
    "print(f\"  Aspect ratio threshold (P{100-FAIL_PERCENTILE_L1}): {ASPECT_RATIO_THRESHOLD:.1f}\")\n",
    "print(f\"  Watertight rate: {df['is_watertight'].mean()*100:.1f}%\")\n",
    "\n",
    "# Apply gate logic based on percentile thresholds\n",
    "print(\"\\nApplying gate logic...\")\n",
    "for idx in df.index:\n",
    "    fail_reasons = []\n",
    "    \n",
    "    if not df.loc[idx, 'has_stl']:\n",
    "        df.loc[idx, 'layer1_pass'] = False\n",
    "        df.loc[idx, 'layer1_fail_reason'] = 'no_stl_file'\n",
    "        continue\n",
    "    \n",
    "    is_watertight = df.loc[idx, 'is_watertight']\n",
    "    non_manifold = df.loc[idx, 'non_manifold_edge_count']\n",
    "    max_aspect = df.loc[idx, 'triangle_aspect_ratio_max']\n",
    "    \n",
    "    # Check metrics against thresholds\n",
    "    if pd.notna(is_watertight) and not is_watertight:\n",
    "        fail_reasons.append('not_watertight')\n",
    "    \n",
    "    if pd.notna(non_manifold) and non_manifold > 0:\n",
    "        fail_reasons.append(f'non_manifold_edges({int(non_manifold)})')\n",
    "    \n",
    "    if pd.notna(max_aspect) and max_aspect > ASPECT_RATIO_THRESHOLD:\n",
    "        fail_reasons.append(f'aspect_ratio({max_aspect:.1f}>{ASPECT_RATIO_THRESHOLD:.1f})')\n",
    "    \n",
    "    # Gate logic: PASS if no more than 1 issue (lenient for AI-generated meshes)\n",
    "    # This allows meshes that are not watertight but have good geometry to pass\n",
    "    df.loc[idx, 'layer1_pass'] = len(fail_reasons) <= 1\n",
    "    df.loc[idx, 'layer1_fail_reason'] = '; '.join(fail_reasons) if fail_reasons else ''\n",
    "\n",
    "# Summary\n",
    "layer1_pass_count = df['layer1_pass'].sum()\n",
    "print(f\"\\nLayer 1 Summary:\")\n",
    "print(f\"  Passed: {layer1_pass_count} ({100*layer1_pass_count/len(df):.1f}%)\")\n",
    "print(f\"  Failed: {len(df) - layer1_pass_count}\")\n",
    "print(f\"\\nMesh statistics:\")\n",
    "print(f\"  Watertight meshes: {df['is_watertight'].sum()}\")\n",
    "print(f\"  Mean triangle count: {df['triangle_count'].mean():.0f}\")\n",
    "print(f\"  Mean edge length: {df['edge_length_mean'].mean():.2f}\")\n",
    "\n",
    "# Note about self-intersection\n",
    "print(\"\\nNote: self_intersection_flag uses broken_faces() as a proxy.\")\n",
    "print(\"Full self-intersection detection requires more sophisticated algorithms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Layer 2: Feature Extractability (Confidence Indicators)\n",
      "============================================================\n",
      "Using percentile-based thresholds (adaptive to dataset):\n",
      "  Symmetry: bottom 25% will fail (lower = worse)\n",
      "  Flatness RMSE: top 75% will fail (higher = worse)\n",
      "  Roundness residual: top 75% will fail (higher = worse)\n",
      "\n",
      "Processing 904 STL files for feature extractability...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 2 - Feature Extractability: 100%|██████████████████████████████████████████████| 904/904 [01:17<00:00, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating percentile-based thresholds from data...\n",
      "  Symmetry threshold (P25): 0.445\n",
      "  Flatness threshold (P75): 6.44 mm\n",
      "  Roundness threshold (P75): 8.77 mm\n",
      "\n",
      "Applying gate logic...\n",
      "\n",
      "Layer 2 Summary:\n",
      "  FAIL: 550 (60.8%)\n",
      "  PASS: 354 (39.2%)\n",
      "\n",
      "Feature extractability statistics:\n",
      "  Mean symmetry confidence: 0.449\n",
      "  Mean hub flatness RMSE: 6.41 mm\n",
      "  Mean bore roundness residual: 8.14 mm\n",
      "\n",
      "Note: bolt_hole_detectability_score is set to NaN (not computed).\n",
      "Reason: Detecting bolt holes requires robust 2D projection and void detection.\n",
      "This is beyond the scope of simple heuristic computation.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 8: Layer 2 - Feature Extractability (Confidence Indicators)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Computing Layer 2: Feature Extractability (Confidence Indicators)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Percentile-based thresholds (will be calculated from data)\n",
    "# Designs below 25th percentile for symmetry OR above 75th percentile for RMSE/residual will fail\n",
    "FAIL_PERCENTILE = 25  # Bottom 25% on each metric flagged as potential issues\n",
    "\n",
    "print(f\"Using percentile-based thresholds (adaptive to dataset):\")\n",
    "print(f\"  Symmetry: bottom {FAIL_PERCENTILE}% will fail (lower = worse)\")\n",
    "print(f\"  Flatness RMSE: top {100-FAIL_PERCENTILE}% will fail (higher = worse)\")\n",
    "print(f\"  Roundness residual: top {100-FAIL_PERCENTILE}% will fail (higher = worse)\")\n",
    "\n",
    "# Initialize Layer 2 columns\n",
    "df['symmetry_axis_confidence'] = np.nan\n",
    "df['hub_plane_flatness_proxy'] = np.nan\n",
    "df['center_bore_roundness_proxy'] = np.nan\n",
    "df['bolt_hole_detectability_score'] = np.nan  # Not computed - too complex\n",
    "df['layer2_status'] = 'UNKNOWN'\n",
    "df['layer2_fail_reason'] = ''\n",
    "\n",
    "def compute_symmetry_confidence(mesh):\n",
    "    \"\"\"Compute rotational symmetry confidence using PCA.\n",
    "    Higher first eigenvalue ratio suggests strong alignment with one axis.\"\"\"\n",
    "    try:\n",
    "        # Sample points from mesh surface\n",
    "        points = mesh.vertices\n",
    "        if len(points) > 5000:\n",
    "            indices = np.random.choice(len(points), 5000, replace=False)\n",
    "            points = points[indices]\n",
    "        \n",
    "        # Center points\n",
    "        centered = points - np.mean(points, axis=0)\n",
    "        \n",
    "        # Compute covariance and eigenvalues\n",
    "        cov = np.cov(centered.T)\n",
    "        eigenvalues = np.linalg.eigvalsh(cov)\n",
    "        eigenvalues = np.sort(eigenvalues)[::-1]  # Descending order\n",
    "        \n",
    "        # Confidence: ratio of largest eigenvalue to sum\n",
    "        # For rotational symmetry (like a wheel), we expect one dominant axis\n",
    "        confidence = eigenvalues[0] / np.sum(eigenvalues)\n",
    "        return confidence\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def compute_hub_flatness(mesh):\n",
    "    \"\"\"Estimate hub plane flatness by fitting plane to points near min-Z region.\"\"\"\n",
    "    try:\n",
    "        points = mesh.vertices\n",
    "        z_coords = points[:, 2]\n",
    "        \n",
    "        # Find points in bottom 10% of Z range (hub region)\n",
    "        z_range = z_coords.max() - z_coords.min()\n",
    "        z_threshold = z_coords.min() + 0.1 * z_range\n",
    "        hub_mask = z_coords <= z_threshold\n",
    "        hub_points = points[hub_mask]\n",
    "        \n",
    "        if len(hub_points) < 10:\n",
    "            return np.nan\n",
    "        \n",
    "        # Fit plane using least squares: z = ax + by + c\n",
    "        A = np.column_stack([hub_points[:, 0], hub_points[:, 1], np.ones(len(hub_points))])\n",
    "        b = hub_points[:, 2]\n",
    "        \n",
    "        # Solve least squares\n",
    "        coeffs, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "        \n",
    "        # Compute RMSE\n",
    "        z_predicted = A @ coeffs\n",
    "        rmse = np.sqrt(np.mean((b - z_predicted) ** 2))\n",
    "        return rmse\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def compute_bore_roundness(mesh):\n",
    "    \"\"\"Estimate center bore roundness by fitting circle to hub slice.\"\"\"\n",
    "    try:\n",
    "        points = mesh.vertices\n",
    "        z_coords = points[:, 2]\n",
    "        \n",
    "        # Get points near hub (bottom 5% of Z)\n",
    "        z_range = z_coords.max() - z_coords.min()\n",
    "        z_threshold = z_coords.min() + 0.05 * z_range\n",
    "        hub_mask = z_coords <= z_threshold\n",
    "        hub_points = points[hub_mask]\n",
    "        \n",
    "        if len(hub_points) < 20:\n",
    "            return np.nan\n",
    "        \n",
    "        # Find center bore: points near the center (inner 30% of radial distance)\n",
    "        xy = hub_points[:, :2]\n",
    "        center_approx = np.mean(xy, axis=0)\n",
    "        radii = np.linalg.norm(xy - center_approx, axis=1)\n",
    "        r_threshold = np.percentile(radii, 30)\n",
    "        bore_mask = radii <= r_threshold\n",
    "        bore_points = xy[bore_mask]\n",
    "        \n",
    "        if len(bore_points) < 10:\n",
    "            return np.nan\n",
    "        \n",
    "        # Fit circle: minimize (r - R)^2 where r = sqrt((x-cx)^2 + (y-cy)^2)\n",
    "        # Use mean as center, compute radius residuals\n",
    "        center = np.mean(bore_points, axis=0)\n",
    "        distances = np.linalg.norm(bore_points - center, axis=1)\n",
    "        mean_radius = np.mean(distances)\n",
    "        residual = np.sqrt(np.mean((distances - mean_radius) ** 2))\n",
    "        return residual\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Process each design with STL\n",
    "stl_indices = df[df['has_stl']].index\n",
    "print(f\"\\nProcessing {len(stl_indices)} STL files for feature extractability...\")\n",
    "\n",
    "for idx in tqdm(stl_indices, desc=\"Layer 2 - Feature Extractability\"):\n",
    "    stl_path = df.loc[idx, 'stl_path']\n",
    "    fail_reasons = []\n",
    "    \n",
    "    try:\n",
    "        mesh = trimesh.load(stl_path)\n",
    "        \n",
    "        # Symmetry confidence\n",
    "        symmetry = compute_symmetry_confidence(mesh)\n",
    "        df.loc[idx, 'symmetry_axis_confidence'] = symmetry\n",
    "        \n",
    "        # Hub flatness\n",
    "        flatness = compute_hub_flatness(mesh)\n",
    "        df.loc[idx, 'hub_plane_flatness_proxy'] = flatness\n",
    "        \n",
    "        # Bore roundness\n",
    "        roundness = compute_bore_roundness(mesh)\n",
    "        df.loc[idx, 'center_bore_roundness_proxy'] = roundness\n",
    "        \n",
    "        # Bolt hole detectability: Not computed (requires robust 2D projection analysis)\n",
    "        df.loc[idx, 'bolt_hole_detectability_score'] = np.nan\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Mark as computation error - will be handled in gate logic below\n",
    "        pass\n",
    "\n",
    "# Calculate percentile-based thresholds from the computed metrics\n",
    "print(\"\\nCalculating percentile-based thresholds from data...\")\n",
    "SYMMETRY_THRESHOLD = df['symmetry_axis_confidence'].quantile(FAIL_PERCENTILE / 100)\n",
    "FLATNESS_THRESHOLD = df['hub_plane_flatness_proxy'].quantile((100 - FAIL_PERCENTILE) / 100)\n",
    "ROUNDNESS_THRESHOLD = df['center_bore_roundness_proxy'].quantile((100 - FAIL_PERCENTILE) / 100)\n",
    "\n",
    "print(f\"  Symmetry threshold (P{FAIL_PERCENTILE}): {SYMMETRY_THRESHOLD:.3f}\")\n",
    "print(f\"  Flatness threshold (P{100-FAIL_PERCENTILE}): {FLATNESS_THRESHOLD:.2f} mm\")\n",
    "print(f\"  Roundness threshold (P{100-FAIL_PERCENTILE}): {ROUNDNESS_THRESHOLD:.2f} mm\")\n",
    "\n",
    "# Apply gate logic based on percentile thresholds\n",
    "print(\"\\nApplying gate logic...\")\n",
    "for idx in df.index:\n",
    "    symmetry = df.loc[idx, 'symmetry_axis_confidence']\n",
    "    flatness = df.loc[idx, 'hub_plane_flatness_proxy']\n",
    "    roundness = df.loc[idx, 'center_bore_roundness_proxy']\n",
    "    \n",
    "    fail_reasons = []\n",
    "    \n",
    "    # Check if all metrics are missing\n",
    "    if pd.isna(symmetry) and pd.isna(flatness) and pd.isna(roundness):\n",
    "        df.loc[idx, 'layer2_status'] = 'UNKNOWN'\n",
    "        if not df.loc[idx, 'has_stl']:\n",
    "            fail_reasons.append('no_stl_file')\n",
    "        else:\n",
    "            fail_reasons.append('all_metrics_unavailable')\n",
    "    else:\n",
    "        issues = []\n",
    "        if not pd.isna(symmetry) and symmetry < SYMMETRY_THRESHOLD:\n",
    "            issues.append(f'low_symmetry({symmetry:.2f}<{SYMMETRY_THRESHOLD:.2f})')\n",
    "        if not pd.isna(flatness) and flatness > FLATNESS_THRESHOLD:\n",
    "            issues.append(f'hub_not_flat({flatness:.1f}>{FLATNESS_THRESHOLD:.1f}mm)')\n",
    "        if not pd.isna(roundness) and roundness > ROUNDNESS_THRESHOLD:\n",
    "            issues.append(f'bore_not_round({roundness:.1f}>{ROUNDNESS_THRESHOLD:.1f}mm)')\n",
    "        \n",
    "        if issues:\n",
    "            df.loc[idx, 'layer2_status'] = 'FAIL'\n",
    "            fail_reasons.extend(issues)\n",
    "        else:\n",
    "            df.loc[idx, 'layer2_status'] = 'PASS'\n",
    "    \n",
    "    df.loc[idx, 'layer2_fail_reason'] = '; '.join(fail_reasons) if fail_reasons else ''\n",
    "\n",
    "# Summary\n",
    "status_counts = df['layer2_status'].value_counts()\n",
    "print(f\"\\nLayer 2 Summary:\")\n",
    "for status, count in status_counts.items():\n",
    "    print(f\"  {status}: {count} ({100*count/len(df):.1f}%)\")\n",
    "\n",
    "print(f\"\\nFeature extractability statistics:\")\n",
    "print(f\"  Mean symmetry confidence: {df['symmetry_axis_confidence'].mean():.3f}\")\n",
    "print(f\"  Mean hub flatness RMSE: {df['hub_plane_flatness_proxy'].mean():.2f} mm\")\n",
    "print(f\"  Mean bore roundness residual: {df['center_bore_roundness_proxy'].mean():.2f} mm\")\n",
    "\n",
    "print(\"\\nNote: bolt_hole_detectability_score is set to NaN (not computed).\")\n",
    "print(\"Reason: Detecting bolt holes requires robust 2D projection and void detection.\")\n",
    "print(\"This is beyond the scope of simple heuristic computation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Layer 3: Physics Plausibility\n",
      "============================================================\n",
      "Thresholds:\n",
      "  Mode7 frequency minimum: 250 Hz (NVH avoidance)\n",
      "  Maximum mass: 25 kg\n",
      "  Minimum frequency ratio (Mode11/Mode7): 2.5\n",
      "\n",
      "Layer 3 Summary:\n",
      "  Passed: 784 (86.7%)\n",
      "  Failed: 120\n",
      "\n",
      "Individual constraint pass rates:\n",
      "  Mass OK (≤25kg): 904 (100.0%)\n",
      "  Mode7 OK (≥250Hz): 904 (100.0%)\n",
      "  Ratio OK (≥2.5): 784 (86.7%)\n",
      "\n",
      "Physics statistics:\n",
      "       modal_separation  frequency_ratio  stiffness_proxy_k7  nvh_margin\n",
      "count        904.000000       904.000000        9.040000e+02  904.000000\n",
      "mean         717.302251         2.712667        1.422377e+08  167.427495\n",
      "std           94.669010         0.156769        2.223034e+07   21.299897\n",
      "min          541.043200         2.383780        9.651935e+07  116.075400\n",
      "25%          647.063450         2.595074        1.251201e+08  152.080750\n",
      "50%          724.682800         2.731676        1.409280e+08  167.026000\n",
      "75%          785.047875         2.826512        1.565976e+08  182.166000\n",
      "max         1039.991300         3.202320        2.135942e+08  229.600300\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 9: Layer 3 - Physics Plausibility (from DeepWheel outputs)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Computing Layer 3: Physics Plausibility\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define physics thresholds\n",
    "MODE7_THRESHOLD = 250  # Hz - NVH avoidance threshold\n",
    "MASS_MAX = 25  # kg - typical passenger vehicle wheel limit\n",
    "RATIO_MIN = 2.5  # minimum frequency ratio for modal separation\n",
    "\n",
    "print(f\"Thresholds:\")\n",
    "print(f\"  Mode7 frequency minimum: {MODE7_THRESHOLD} Hz (NVH avoidance)\")\n",
    "print(f\"  Maximum mass: {MASS_MAX} kg\")\n",
    "print(f\"  Minimum frequency ratio (Mode11/Mode7): {RATIO_MIN}\")\n",
    "\n",
    "# Compute derived physics features\n",
    "df['modal_separation'] = df['mode11_freq'] - df['mode7_freq']\n",
    "df['frequency_ratio'] = df['mode11_freq'] / df['mode7_freq']\n",
    "\n",
    "# Stiffness proxy: k = m * (2*pi*f)^2\n",
    "df['stiffness_proxy_k7'] = df['mass'] * (2 * np.pi * df['mode7_freq']) ** 2\n",
    "df['stiffness_proxy_k11'] = df['mass'] * (2 * np.pi * df['mode11_freq']) ** 2\n",
    "\n",
    "# NVH margin\n",
    "df['nvh_margin'] = df['mode7_freq'] - MODE7_THRESHOLD\n",
    "\n",
    "# Layer 3 gate logic\n",
    "df['layer3_mass_ok'] = df['mass'] <= MASS_MAX\n",
    "df['layer3_mode7_ok'] = df['mode7_freq'] >= MODE7_THRESHOLD\n",
    "df['layer3_ratio_ok'] = df['frequency_ratio'] >= RATIO_MIN\n",
    "\n",
    "# Build failure reasons\n",
    "def build_layer3_fail_reason(row):\n",
    "    reasons = []\n",
    "    if not row['layer3_mass_ok']:\n",
    "        reasons.append(f\"mass_exceeded({row['mass']:.2f}kg)\")\n",
    "    if not row['layer3_mode7_ok']:\n",
    "        reasons.append(f\"mode7_below_threshold({row['mode7_freq']:.1f}Hz)\")\n",
    "    if not row['layer3_ratio_ok']:\n",
    "        reasons.append(f\"ratio_below_min({row['frequency_ratio']:.2f})\")\n",
    "    return '; '.join(reasons)\n",
    "\n",
    "df['layer3_pass'] = df['layer3_mass_ok'] & df['layer3_mode7_ok'] & df['layer3_ratio_ok']\n",
    "df['layer3_fail_reason'] = df.apply(build_layer3_fail_reason, axis=1)\n",
    "\n",
    "# Summary\n",
    "layer3_pass_count = df['layer3_pass'].sum()\n",
    "print(f\"\\nLayer 3 Summary:\")\n",
    "print(f\"  Passed: {layer3_pass_count} ({100*layer3_pass_count/len(df):.1f}%)\")\n",
    "print(f\"  Failed: {len(df) - layer3_pass_count}\")\n",
    "\n",
    "print(f\"\\nIndividual constraint pass rates:\")\n",
    "print(f\"  Mass OK (≤{MASS_MAX}kg): {df['layer3_mass_ok'].sum()} ({100*df['layer3_mass_ok'].mean():.1f}%)\")\n",
    "print(f\"  Mode7 OK (≥{MODE7_THRESHOLD}Hz): {df['layer3_mode7_ok'].sum()} ({100*df['layer3_mode7_ok'].mean():.1f}%)\")\n",
    "print(f\"  Ratio OK (≥{RATIO_MIN}): {df['layer3_ratio_ok'].sum()} ({100*df['layer3_ratio_ok'].mean():.1f}%)\")\n",
    "\n",
    "print(f\"\\nPhysics statistics:\")\n",
    "print(df[['modal_separation', 'frequency_ratio', 'stiffness_proxy_k7', 'nvh_margin']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Layer 4: Engineering Constraint Evaluation\n",
      "============================================================\n",
      "Thresholds:\n",
      "  Mass: ≤ 25 kg\n",
      "  Mode7 frequency: ≥ 250 Hz\n",
      "  Frequency ratio: ≥ 2.5\n",
      "  Stiffness (k7): ≥ 121697716 N/m (10th percentile of quality subset)\n",
      "\n",
      "Layer 4 Summary:\n",
      "  Passed all constraints: 705 (78.0%)\n",
      "  Failed at least one: 199\n",
      "\n",
      "Violation count distribution:\n",
      "  0 violations: 705 designs (78.0%)\n",
      "  1 violations: 108 designs (11.9%)\n",
      "  2 violations: 91 designs (10.1%)\n",
      "\n",
      "Individual constraint pass rates:\n",
      "  mass_ok: 904 (100.0%)\n",
      "  mode7_ok: 904 (100.0%)\n",
      "  ratio_ok: 784 (86.7%)\n",
      "  stiffness_ok: 734 (81.2%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 10: Layer 4 - Engineering Constraint Evaluation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Computing Layer 4: Engineering Constraint Evaluation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define stiffness threshold based on percentile of high-quality designs\n",
    "# Use 10th percentile of stiffness among designs that pass Layer 3\n",
    "quality_subset = df[df['layer3_pass']]['stiffness_proxy_k7']\n",
    "if len(quality_subset) > 0:\n",
    "    STIFFNESS_THRESHOLD = quality_subset.quantile(0.10)\n",
    "else:\n",
    "    # Fallback: use 10th percentile of all designs\n",
    "    STIFFNESS_THRESHOLD = df['stiffness_proxy_k7'].quantile(0.10)\n",
    "\n",
    "print(f\"Thresholds:\")\n",
    "print(f\"  Mass: ≤ {MASS_MAX} kg\")\n",
    "print(f\"  Mode7 frequency: ≥ {MODE7_THRESHOLD} Hz\")\n",
    "print(f\"  Frequency ratio: ≥ {RATIO_MIN}\")\n",
    "print(f\"  Stiffness (k7): ≥ {STIFFNESS_THRESHOLD:.0f} N/m (10th percentile of quality subset)\")\n",
    "\n",
    "# Compute constraint flags\n",
    "df['mass_ok'] = df['mass'] <= MASS_MAX\n",
    "df['mode7_ok'] = df['mode7_freq'] >= MODE7_THRESHOLD\n",
    "df['ratio_ok'] = df['frequency_ratio'] >= RATIO_MIN\n",
    "df['stiffness_ok'] = df['stiffness_proxy_k7'] >= STIFFNESS_THRESHOLD\n",
    "\n",
    "# Aggregate metrics\n",
    "constraint_cols = ['mass_ok', 'mode7_ok', 'ratio_ok', 'stiffness_ok']\n",
    "df['constraint_violation_count'] = (~df[constraint_cols]).sum(axis=1)\n",
    "df['passes_all_constraints'] = df[constraint_cols].all(axis=1)\n",
    "\n",
    "# Build failure reasons\n",
    "def build_layer4_fail_reason(row):\n",
    "    reasons = []\n",
    "    if not row['mass_ok']:\n",
    "        reasons.append(f\"mass({row['mass']:.2f}>{MASS_MAX})\")\n",
    "    if not row['mode7_ok']:\n",
    "        reasons.append(f\"mode7({row['mode7_freq']:.1f}<{MODE7_THRESHOLD})\")\n",
    "    if not row['ratio_ok']:\n",
    "        reasons.append(f\"ratio({row['frequency_ratio']:.2f}<{RATIO_MIN})\")\n",
    "    if not row['stiffness_ok']:\n",
    "        reasons.append(f\"stiffness({row['stiffness_proxy_k7']:.0f}<{STIFFNESS_THRESHOLD:.0f})\")\n",
    "    return '; '.join(reasons)\n",
    "\n",
    "df['layer4_pass'] = df['passes_all_constraints']\n",
    "df['layer4_fail_reason'] = df.apply(build_layer4_fail_reason, axis=1)\n",
    "\n",
    "# Summary\n",
    "layer4_pass_count = df['layer4_pass'].sum()\n",
    "print(f\"\\nLayer 4 Summary:\")\n",
    "print(f\"  Passed all constraints: {layer4_pass_count} ({100*layer4_pass_count/len(df):.1f}%)\")\n",
    "print(f\"  Failed at least one: {len(df) - layer4_pass_count}\")\n",
    "\n",
    "print(f\"\\nViolation count distribution:\")\n",
    "violation_dist = df['constraint_violation_count'].value_counts().sort_index()\n",
    "for count, n in violation_dist.items():\n",
    "    print(f\"  {count} violations: {n} designs ({100*n/len(df):.1f}%)\")\n",
    "\n",
    "print(f\"\\nIndividual constraint pass rates:\")\n",
    "for col in constraint_cols:\n",
    "    print(f\"  {col}: {df[col].sum()} ({100*df[col].mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Layer 5: Manufacturability & CAD Readiness\n",
      "============================================================\n",
      "Thresholds:\n",
      "  Dihedral angle for sharp edge: > 150 degrees\n",
      "  Max sharp edge density: 0.1 edges/mm^2\n",
      "\n",
      "Processing 904 STL files for manufacturability...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 5 - Manufacturability: 100%|███████████████████████████████████████████████████| 904/904 [04:10<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer 5 Summary:\n",
      "  PASS: 884 (97.8%)\n",
      "  UNKNOWN: 20 (2.2%)\n",
      "\n",
      "Manufacturability statistics:\n",
      "  Mean wall thickness est: 4.18 mm\n",
      "  Mean sharp edge density: 0.000029\n",
      "  Mean CAD readiness score: 0.73\n",
      "\n",
      "Note: undercut_risk_score is set to NaN (not computed).\n",
      "Reason: Undercut detection requires visibility analysis from mold direction.\n",
      "This is beyond the scope of simple mesh-based heuristics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 11: Layer 5 - Manufacturability & CAD Readiness (Heuristics)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Computing Layer 5: Manufacturability & CAD Readiness\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Thresholds\n",
    "DIHEDRAL_ANGLE_THRESHOLD = 150  # degrees, edges above this are \"sharp\"\n",
    "SHARP_EDGE_DENSITY_MAX = 0.1  # max acceptable sharp edges per mm^2\n",
    "\n",
    "print(f\"Thresholds:\")\n",
    "print(f\"  Dihedral angle for sharp edge: > {DIHEDRAL_ANGLE_THRESHOLD} degrees\")\n",
    "print(f\"  Max sharp edge density: {SHARP_EDGE_DENSITY_MAX} edges/mm^2\")\n",
    "\n",
    "# Initialize Layer 5 columns\n",
    "df['min_wall_thickness_est'] = np.nan  # Complex computation\n",
    "df['sharp_edge_density'] = np.nan\n",
    "df['fillet_feasibility_score'] = np.nan\n",
    "df['undercut_risk_score'] = np.nan  # Not computed\n",
    "df['balance_uniformity'] = np.nan\n",
    "df['cad_readiness_score'] = np.nan\n",
    "df['layer5_status'] = 'UNKNOWN'\n",
    "df['layer5_reason'] = ''\n",
    "\n",
    "def compute_sharp_edge_density(mesh):\n",
    "    \"\"\"Compute sharp edge density: count edges with dihedral angle > threshold.\"\"\"\n",
    "    try:\n",
    "        # Get face adjacency for dihedral angles\n",
    "        face_adjacency = mesh.face_adjacency\n",
    "        face_adjacency_angles = mesh.face_adjacency_angles\n",
    "        \n",
    "        # Convert to degrees\n",
    "        angles_deg = np.degrees(face_adjacency_angles)\n",
    "        \n",
    "        # Count sharp edges (dihedral angle > threshold)\n",
    "        sharp_edges = np.sum(angles_deg > DIHEDRAL_ANGLE_THRESHOLD)\n",
    "        \n",
    "        # Normalize by surface area\n",
    "        surface_area = mesh.area\n",
    "        if surface_area > 0:\n",
    "            density = sharp_edges / surface_area\n",
    "        else:\n",
    "            density = np.nan\n",
    "        \n",
    "        return density, sharp_edges\n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def compute_balance_uniformity(mesh):\n",
    "    \"\"\"Compute balance uniformity using radial mass distribution proxy.\"\"\"\n",
    "    try:\n",
    "        points = mesh.vertices\n",
    "        \n",
    "        # Find approximate axis (assume Z is rotation axis)\n",
    "        center = np.mean(points, axis=0)\n",
    "        \n",
    "        # Compute radial distances from center (in XY plane)\n",
    "        xy = points[:, :2] - center[:2]\n",
    "        radii = np.linalg.norm(xy, axis=1)\n",
    "        \n",
    "        # Coefficient of variation of radial distances\n",
    "        if np.mean(radii) > 0:\n",
    "            cv = np.std(radii) / np.mean(radii)\n",
    "        else:\n",
    "            cv = np.nan\n",
    "        \n",
    "        return cv\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def compute_wall_thickness_proxy(mesh):\n",
    "    \"\"\"Estimate minimum wall thickness using KDTree nearest neighbor.\"\"\"\n",
    "    try:\n",
    "        from scipy.spatial import KDTree\n",
    "        \n",
    "        # Sample surface points\n",
    "        points = mesh.vertices\n",
    "        normals = mesh.vertex_normals\n",
    "        \n",
    "        if len(points) > 2000:\n",
    "            indices = np.random.choice(len(points), 2000, replace=False)\n",
    "            points = points[indices]\n",
    "            normals = normals[indices]\n",
    "        \n",
    "        # Build KDTree\n",
    "        tree = KDTree(points)\n",
    "        \n",
    "        # For each point, find nearest point in opposite direction\n",
    "        # (approximate: find nearest point not too close in normal direction)\n",
    "        thicknesses = []\n",
    "        for i, (p, n) in enumerate(zip(points, normals)):\n",
    "            # Query multiple nearest neighbors\n",
    "            distances, indices = tree.query(p, k=min(20, len(points)))\n",
    "            \n",
    "            # Find nearest point that's in roughly opposite normal direction\n",
    "            for d, idx in zip(distances[1:], indices[1:]):  # Skip self\n",
    "                if d > 0:\n",
    "                    direction = points[idx] - p\n",
    "                    direction = direction / np.linalg.norm(direction)\n",
    "                    # Check if opposite direction (dot product with normal < 0)\n",
    "                    if np.dot(direction, n) < -0.5:  # Roughly opposite\n",
    "                        thicknesses.append(d)\n",
    "                        break\n",
    "        \n",
    "        if thicknesses:\n",
    "            return np.min(thicknesses)\n",
    "        else:\n",
    "            return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Process each design with STL\n",
    "stl_indices = df[df['has_stl']].index\n",
    "print(f\"\\nProcessing {len(stl_indices)} STL files for manufacturability...\")\n",
    "\n",
    "for idx in tqdm(stl_indices, desc=\"Layer 5 - Manufacturability\"):\n",
    "    stl_path = df.loc[idx, 'stl_path']\n",
    "    fail_reasons = []\n",
    "    \n",
    "    try:\n",
    "        mesh = trimesh.load(stl_path)\n",
    "        \n",
    "        # Wall thickness (simplified proxy)\n",
    "        thickness = compute_wall_thickness_proxy(mesh)\n",
    "        df.loc[idx, 'min_wall_thickness_est'] = thickness\n",
    "        \n",
    "        # Sharp edge density\n",
    "        density, _ = compute_sharp_edge_density(mesh)\n",
    "        df.loc[idx, 'sharp_edge_density'] = density\n",
    "        \n",
    "        # Fillet feasibility (inverse of sharp edge density)\n",
    "        if not pd.isna(density) and density > 0:\n",
    "            # Normalize: 1 means no sharp edges, 0 means max sharp edges\n",
    "            fillet_score = max(0, 1 - density / SHARP_EDGE_DENSITY_MAX)\n",
    "            df.loc[idx, 'fillet_feasibility_score'] = fillet_score\n",
    "        \n",
    "        # Undercut risk: Not computed (requires visibility analysis)\n",
    "        df.loc[idx, 'undercut_risk_score'] = np.nan\n",
    "        \n",
    "        # Balance uniformity\n",
    "        uniformity = compute_balance_uniformity(mesh)\n",
    "        df.loc[idx, 'balance_uniformity'] = uniformity\n",
    "        \n",
    "        # CAD readiness score: weighted average with nuanced mesh validity\n",
    "        # Weights: mesh validity (0.25), feature extractability (0.35), manufacturability (0.40)\n",
    "        # Use nuanced mesh score (accounts for AI mesh quality issues)\n",
    "        mesh_issues = 0\n",
    "        if not df.loc[idx, 'is_watertight']:\n",
    "            mesh_issues += 1\n",
    "        if df.loc[idx, 'non_manifold_edge_count'] > 0:\n",
    "            mesh_issues += 1\n",
    "        mesh_valid = max(0.3, 1.0 - (mesh_issues * 0.25))  # Min 0.3 for AI meshes\n",
    "        \n",
    "        feat_extract = 0.5  # Default neutral\n",
    "        if df.loc[idx, 'layer2_status'] == 'PASS':\n",
    "            feat_extract = 1.0\n",
    "        elif df.loc[idx, 'layer2_status'] == 'FAIL':\n",
    "            feat_extract = 0.35  # Not 0, since these are soft indicators\n",
    "        \n",
    "        manuf_score = df.loc[idx, 'fillet_feasibility_score']\n",
    "        if pd.isna(manuf_score):\n",
    "            manuf_score = 0.5  # Neutral if unknown\n",
    "        \n",
    "        cad_score = 0.25 * mesh_valid + 0.35 * feat_extract + 0.40 * manuf_score\n",
    "        df.loc[idx, 'cad_readiness_score'] = cad_score\n",
    "        \n",
    "        # Gate logic - adjusted thresholds for AI-generated meshes\n",
    "        if cad_score >= 0.50:\n",
    "            df.loc[idx, 'layer5_status'] = 'PASS'\n",
    "        elif cad_score >= 0.35:\n",
    "            df.loc[idx, 'layer5_status'] = 'UNKNOWN'\n",
    "            fail_reasons.append(f'marginal_cad_score({cad_score:.2f})')\n",
    "        else:\n",
    "            df.loc[idx, 'layer5_status'] = 'FAIL'\n",
    "            fail_reasons.append(f'low_cad_score({cad_score:.2f})')\n",
    "        \n",
    "        df.loc[idx, 'layer5_reason'] = '; '.join(fail_reasons) if fail_reasons else ''\n",
    "        \n",
    "    except Exception as e:\n",
    "        df.loc[idx, 'layer5_status'] = 'UNKNOWN'\n",
    "        df.loc[idx, 'layer5_reason'] = f\"computation_error: {str(e)[:50]}\"\n",
    "\n",
    "# Handle designs without STL\n",
    "no_stl_mask = ~df['has_stl']\n",
    "df.loc[no_stl_mask, 'layer5_status'] = 'UNKNOWN'\n",
    "df.loc[no_stl_mask, 'layer5_reason'] = 'no_stl_file'\n",
    "\n",
    "# Summary\n",
    "status_counts = df['layer5_status'].value_counts()\n",
    "print(f\"\\nLayer 5 Summary:\")\n",
    "for status, count in status_counts.items():\n",
    "    print(f\"  {status}: {count} ({100*count/len(df):.1f}%)\")\n",
    "\n",
    "print(f\"\\nManufacturability statistics:\")\n",
    "print(f\"  Mean wall thickness est: {df['min_wall_thickness_est'].mean():.2f} mm\")\n",
    "print(f\"  Mean sharp edge density: {df['sharp_edge_density'].mean():.6f}\")\n",
    "print(f\"  Mean CAD readiness score: {df['cad_readiness_score'].mean():.2f}\")\n",
    "\n",
    "print(\"\\nNote: undercut_risk_score is set to NaN (not computed).\")\n",
    "print(\"Reason: Undercut detection requires visibility analysis from mold direction.\")\n",
    "print(\"This is beyond the scope of simple mesh-based heuristics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Layer 6: Analytics & ML Readiness\n",
      "============================================================\n",
      "Risk levels will be assigned using percentile-based thresholds\n",
      "\n",
      "Percentile-based risk thresholds:\n",
      "  High risk: margin < 0.036 (P33)\n",
      "  Medium risk: 0.036 ≤ margin < 0.105 (P33-P67)\n",
      "  Low risk: margin ≥ 0.105 (P67+)\n",
      "\n",
      "Layer 6 Summary:\n",
      "  Medium risk: 308 (34.1%)\n",
      "  Low risk: 298 (33.0%)\n",
      "  High risk: 298 (33.0%)\n",
      "\n",
      "Design margin statistics:\n",
      "count    904.000000\n",
      "mean       0.055062\n",
      "std        0.077918\n",
      "min       -0.206893\n",
      "25%        0.010692\n",
      "50%        0.078798\n",
      "75%        0.115453\n",
      "max        0.167055\n",
      "Name: design_margin_score, dtype: float64\n",
      "\n",
      "Z-score ranges:\n",
      "  z_mass: [-2.54, 3.18]\n",
      "  z_mode7: [-2.41, 2.92]\n",
      "  z_frequency_ratio: [-2.10, 3.12]\n",
      "\n",
      "Note: vehicle_class_label is set to UNKNOWN for all designs.\n",
      "Reason: Wheel diameter cannot be reliably extracted from mesh geometry alone.\n",
      "Proper vehicle class assignment requires additional metadata or consistent mesh orientation.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 12: Layer 6 - Analytics & ML Readiness\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Computing Layer 6: Analytics & ML Readiness\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Risk level thresholds - using percentile-based approach for balanced distribution\n",
    "# Will be calculated after design_margin_score is computed\n",
    "print(\"Risk levels will be assigned using percentile-based thresholds\")\n",
    "\n",
    "# Normalized features (min-max normalization)\n",
    "df['normalized_mass'] = (df['mass'] - df['mass'].min()) / (df['mass'].max() - df['mass'].min())\n",
    "df['normalized_mode7'] = (df['mode7_freq'] - df['mode7_freq'].min()) / (df['mode7_freq'].max() - df['mode7_freq'].min())\n",
    "\n",
    "# Z-score normalization\n",
    "df['z_mass'] = (df['mass'] - df['mass'].mean()) / df['mass'].std()\n",
    "df['z_mode7'] = (df['mode7_freq'] - df['mode7_freq'].mean()) / df['mode7_freq'].std()\n",
    "df['z_mode11'] = (df['mode11_freq'] - df['mode11_freq'].mean()) / df['mode11_freq'].std()\n",
    "df['z_frequency_ratio'] = (df['frequency_ratio'] - df['frequency_ratio'].mean()) / df['frequency_ratio'].std()\n",
    "\n",
    "# Margin calculations (normalized distance to constraint boundary)\n",
    "# Higher margin = better (further from failure)\n",
    "df['mass_margin_norm'] = (MASS_MAX - df['mass']) / MASS_MAX\n",
    "df['mode7_margin_norm'] = (df['mode7_freq'] - MODE7_THRESHOLD) / MODE7_THRESHOLD\n",
    "df['ratio_margin_norm'] = (df['frequency_ratio'] - RATIO_MIN) / RATIO_MIN\n",
    "\n",
    "# Stiffness margin (normalized to threshold)\n",
    "df['stiffness_margin_norm'] = (df['stiffness_proxy_k7'] - STIFFNESS_THRESHOLD) / STIFFNESS_THRESHOLD\n",
    "\n",
    "# Design margin score: minimum of all normalized margins\n",
    "margin_cols = ['mass_margin_norm', 'mode7_margin_norm', 'ratio_margin_norm', 'stiffness_margin_norm']\n",
    "df['design_margin_score'] = df[margin_cols].min(axis=1)\n",
    "\n",
    "# Risk level classification using percentile-based thresholds\n",
    "# This ensures a balanced distribution across risk categories\n",
    "RISK_HIGH_THRESHOLD = df['design_margin_score'].quantile(0.33)  # Bottom 33%\n",
    "RISK_MEDIUM_THRESHOLD = df['design_margin_score'].quantile(0.67)  # Middle 33%\n",
    "\n",
    "print(f\"\\nPercentile-based risk thresholds:\")\n",
    "print(f\"  High risk: margin < {RISK_HIGH_THRESHOLD:.3f} (P33)\")\n",
    "print(f\"  Medium risk: {RISK_HIGH_THRESHOLD:.3f} ≤ margin < {RISK_MEDIUM_THRESHOLD:.3f} (P33-P67)\")\n",
    "print(f\"  Low risk: margin ≥ {RISK_MEDIUM_THRESHOLD:.3f} (P67+)\")\n",
    "\n",
    "def classify_risk(margin):\n",
    "    if pd.isna(margin):\n",
    "        return 'UNKNOWN'\n",
    "    elif margin < RISK_HIGH_THRESHOLD:\n",
    "        return 'High'\n",
    "    elif margin < RISK_MEDIUM_THRESHOLD:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "df['risk_level'] = df['design_margin_score'].apply(classify_risk)\n",
    "\n",
    "# Vehicle class label: Cannot determine without diameter\n",
    "# Diameter is not directly available from mesh (would need consistent orientation)\n",
    "# Set as UNKNOWN with documentation\n",
    "df['vehicle_class_label'] = 'UNKNOWN'\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nLayer 6 Summary:\")\n",
    "risk_dist = df['risk_level'].value_counts()\n",
    "for level, count in risk_dist.items():\n",
    "    print(f\"  {level} risk: {count} ({100*count/len(df):.1f}%)\")\n",
    "\n",
    "print(f\"\\nDesign margin statistics:\")\n",
    "print(df['design_margin_score'].describe())\n",
    "\n",
    "print(f\"\\nZ-score ranges:\")\n",
    "for col in ['z_mass', 'z_mode7', 'z_frequency_ratio']:\n",
    "    print(f\"  {col}: [{df[col].min():.2f}, {df[col].max():.2f}]\")\n",
    "\n",
    "print(\"\\nNote: vehicle_class_label is set to UNKNOWN for all designs.\")\n",
    "print(\"Reason: Wheel diameter cannot be reliably extracted from mesh geometry alone.\")\n",
    "print(\"Proper vehicle class assignment requires additional metadata or consistent mesh orientation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling final dataset...\n",
      "============================================================\n",
      "\n",
      "Saved feature dataset to: C:\\Users\\mahil.kr\\GL\\data-analytics-capstone\\data\\processed\\deepwheel_features_full.csv\n",
      "  Total rows: 904\n",
      "  Total columns: 74\n",
      "\n",
      "Column groups:\n",
      "  Raw/Original: 7\n",
      "  File paths: 4\n",
      "  Layer 0: 8\n",
      "  Layer 1: 8\n",
      "  Layer 2: 6\n",
      "  Layer 3: 11\n",
      "  Layer 4: 11\n",
      "  Layer 5: 8\n",
      "  Layer 6: 15\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 13: Assemble Final Dataset + Ordering\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Assembling final dataset...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define column ordering by layer\n",
    "column_order = [\n",
    "    # Original/Raw columns\n",
    "    'file_name',\n",
    "    'mass', 'mass_original',\n",
    "    'mode7_freq', 'mode7_freq_original',\n",
    "    'mode11_freq', 'mode11_freq_original',\n",
    "    \n",
    "    # File paths\n",
    "    'stl_path', 'step_path', 'has_stl', 'has_step',\n",
    "    \n",
    "    # Layer 0: Data Integrity & Scale\n",
    "    'bbox_x', 'bbox_y', 'bbox_z', 'bbox_volume_proxy',\n",
    "    'unit_scale_flag', 'axis_orientation_flag',\n",
    "    'layer0_pass', 'layer0_fail_reason',\n",
    "    \n",
    "    # Layer 1: Mesh Validity\n",
    "    'triangle_count', 'is_watertight', 'non_manifold_edge_count',\n",
    "    'self_intersection_flag', 'normal_consistency_flag',\n",
    "    'edge_length_mean', 'edge_length_std', 'triangle_aspect_ratio_max',\n",
    "    'layer1_pass', 'layer1_fail_reason',\n",
    "    \n",
    "    # Layer 2: Feature Extractability\n",
    "    'symmetry_axis_confidence', 'hub_plane_flatness_proxy',\n",
    "    'center_bore_roundness_proxy', 'bolt_hole_detectability_score',\n",
    "    'layer2_status', 'layer2_fail_reason',\n",
    "    \n",
    "    # Layer 3: Physics Plausibility\n",
    "    'modal_separation', 'frequency_ratio',\n",
    "    'stiffness_proxy_k7', 'stiffness_proxy_k11', 'nvh_margin',\n",
    "    'layer3_mass_ok', 'layer3_mode7_ok', 'layer3_ratio_ok',\n",
    "    'layer3_pass', 'layer3_fail_reason',\n",
    "    \n",
    "    # Layer 4: Engineering Constraints\n",
    "    'mass_ok', 'mode7_ok', 'ratio_ok', 'stiffness_ok',\n",
    "    'constraint_violation_count', 'passes_all_constraints',\n",
    "    'layer4_pass', 'layer4_fail_reason',\n",
    "    \n",
    "    # Layer 5: Manufacturability\n",
    "    'min_wall_thickness_est', 'sharp_edge_density',\n",
    "    'fillet_feasibility_score', 'undercut_risk_score', 'balance_uniformity',\n",
    "    'cad_readiness_score', 'layer5_status', 'layer5_reason',\n",
    "    \n",
    "    # Layer 6: Analytics & ML Readiness\n",
    "    'normalized_mass', 'normalized_mode7',\n",
    "    'z_mass', 'z_mode7', 'z_mode11', 'z_frequency_ratio',\n",
    "    'mass_margin_norm', 'mode7_margin_norm', 'ratio_margin_norm', 'stiffness_margin_norm',\n",
    "    'design_margin_score', 'risk_level', 'vehicle_class_label'\n",
    "]\n",
    "\n",
    "# Verify all columns exist and add any missing ones\n",
    "existing_cols = set(df.columns)\n",
    "ordered_cols = [col for col in column_order if col in existing_cols]\n",
    "missing_from_order = existing_cols - set(column_order)\n",
    "missing_from_df = set(column_order) - existing_cols\n",
    "\n",
    "if missing_from_order:\n",
    "    print(f\"Note: Columns not in ordering (will be appended): {missing_from_order}\")\n",
    "    ordered_cols.extend(sorted(missing_from_order))\n",
    "\n",
    "if missing_from_df:\n",
    "    print(f\"Warning: Columns in ordering but not in dataframe: {missing_from_df}\")\n",
    "\n",
    "# Reorder columns\n",
    "df_final = df[ordered_cols].copy()\n",
    "\n",
    "# Save to CSV\n",
    "df_final.to_csv(OUTPUT_FEATURES_CSV, index=False)\n",
    "print(f\"\\nSaved feature dataset to: {OUTPUT_FEATURES_CSV}\")\n",
    "print(f\"  Total rows: {len(df_final)}\")\n",
    "print(f\"  Total columns: {len(df_final.columns)}\")\n",
    "\n",
    "# Show column groups\n",
    "print(f\"\\nColumn groups:\")\n",
    "print(f\"  Raw/Original: {len([c for c in ordered_cols if 'original' in c or c in ['file_name', 'mass', 'mode7_freq', 'mode11_freq']])}\")\n",
    "print(f\"  File paths: {len([c for c in ordered_cols if 'path' in c or 'has_' in c])}\")\n",
    "print(f\"  Layer 0: {len([c for c in ordered_cols if 'layer0' in c or 'bbox' in c or 'scale' in c or 'orientation' in c])}\")\n",
    "print(f\"  Layer 1: {len([c for c in ordered_cols if 'layer1' in c or 'triangle' in c or 'watertight' in c or 'manifold' in c or 'edge_length' in c])}\")\n",
    "print(f\"  Layer 2: {len([c for c in ordered_cols if 'layer2' in c or 'symmetry' in c or 'flatness' in c or 'roundness' in c or 'bolt_hole' in c])}\")\n",
    "print(f\"  Layer 3: {len([c for c in ordered_cols if 'layer3' in c or 'modal' in c or 'frequency_ratio' in c or 'stiffness_proxy' in c or 'nvh' in c])}\")\n",
    "print(f\"  Layer 4: {len([c for c in ordered_cols if 'layer4' in c or '_ok' in c or 'constraint' in c or 'passes_all' in c])}\")\n",
    "print(f\"  Layer 5: {len([c for c in ordered_cols if 'layer5' in c or 'thickness' in c or 'sharp' in c or 'fillet' in c or 'undercut' in c or 'balance' in c or 'cad_' in c])}\")\n",
    "print(f\"  Layer 6: {len([c for c in ordered_cols if 'normalized' in c or 'z_' in c or 'margin' in c or 'risk' in c or 'vehicle' in c])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data dictionary...\n",
      "============================================================\n",
      "Saved data dictionary to: C:\\Users\\mahil.kr\\GL\\data-analytics-capstone\\data\\processed\\deepwheel_data_dictionary.csv\n",
      "  Total entries: 74\n",
      "\n",
      "Entries by layer:\n",
      "  Layer raw: 11 columns\n",
      "  Layer 0: 8 columns\n",
      "  Layer 1: 10 columns\n",
      "  Layer 2: 6 columns\n",
      "  Layer 3: 10 columns\n",
      "  Layer 4: 8 columns\n",
      "  Layer 5: 8 columns\n",
      "  Layer 6: 13 columns\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 14: Create Data Dictionary CSV\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Creating data dictionary...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define data dictionary entries\n",
    "data_dict_entries = [\n",
    "    # Raw columns\n",
    "    ('file_name', 'raw', 'Unique identifier for each wheel design', 'string', '-', 'raw', 'From DeepWheel simulation', 'alphanumeric string'),\n",
    "    ('mass', 'raw', 'Wheel mass from simulation', 'float', 'kg', 'raw', 'Direct from DeepWheel', '15-30 kg typical'),\n",
    "    ('mass_original', 'raw', 'Original mass value (backup)', 'float', 'kg', 'raw', 'Copy of mass column', '15-30 kg typical'),\n",
    "    ('mode7_freq', 'raw', '7th mode natural frequency', 'float', 'Hz', 'raw', 'DeepWheel modal analysis', '300-500 Hz typical'),\n",
    "    ('mode7_freq_original', 'raw', 'Original Mode7 frequency (backup)', 'float', 'Hz', 'raw', 'Copy of mode7_freq', '300-500 Hz typical'),\n",
    "    ('mode11_freq', 'raw', '11th mode natural frequency', 'float', 'Hz', 'raw', 'DeepWheel modal analysis', '900-1400 Hz typical'),\n",
    "    ('mode11_freq_original', 'raw', 'Original Mode11 frequency (backup)', 'float', 'Hz', 'raw', 'Copy of mode11_freq', '900-1400 Hz typical'),\n",
    "    \n",
    "    # File paths\n",
    "    ('stl_path', 'raw', 'Path to STL mesh file', 'string', '-', 'derived', 'data/stl/{file_name}.stl', 'file path or null'),\n",
    "    ('step_path', 'raw', 'Path to STEP CAD file', 'string', '-', 'derived', 'data/step/{file_name}.stp', 'file path or null'),\n",
    "    ('has_stl', 'raw', 'Whether STL file exists', 'boolean', '-', 'derived', 'stl_path is not null', 'True/False'),\n",
    "    ('has_step', 'raw', 'Whether STEP file exists', 'boolean', '-', 'derived', 'step_path is not null', 'True/False'),\n",
    "    \n",
    "    # Layer 0\n",
    "    ('bbox_x', 0, 'Bounding box X dimension', 'float', 'mm', 'mesh', 'max(x) - min(x) from STL vertices', '100-600 mm expected'),\n",
    "    ('bbox_y', 0, 'Bounding box Y dimension', 'float', 'mm', 'mesh', 'max(y) - min(y) from STL vertices', '100-600 mm expected'),\n",
    "    ('bbox_z', 0, 'Bounding box Z dimension', 'float', 'mm', 'mesh', 'max(z) - min(z) from STL vertices', '100-600 mm expected'),\n",
    "    ('bbox_volume_proxy', 0, 'Bounding box volume proxy', 'float', 'mm^3', 'mesh', 'bbox_x * bbox_y * bbox_z', 'varies'),\n",
    "    ('unit_scale_flag', 0, 'Flag for out-of-scale dimensions', 'boolean', '-', 'derived', 'any bbox dim < 100 or > 600 mm', 'True=problem'),\n",
    "    ('axis_orientation_flag', 0, 'Flag for incorrect axis orientation', 'boolean', '-', 'derived', 'True if Z is not the smallest bbox dimension', 'True=problem, False=OK'),\n",
    "    ('layer0_pass', 0, 'Layer 0 gate pass status', 'boolean', '-', 'derived', 'unit_scale_flag == False AND axis_orientation_flag == False', 'True/False'),\n",
    "    ('layer0_fail_reason', 0, 'Reason for Layer 0 failure', 'string', '-', 'derived', 'Concatenated failure reasons', 'text or empty'),\n",
    "    \n",
    "    # Layer 1\n",
    "    ('triangle_count', 1, 'Number of triangles in mesh', 'int', '-', 'mesh', 'len(mesh.faces)', 'varies'),\n",
    "    ('is_watertight', 1, 'Whether mesh is watertight', 'boolean', '-', 'mesh', 'mesh.is_watertight', 'True/False'),\n",
    "    ('non_manifold_edge_count', 1, 'Count of non-manifold edges', 'int', '-', 'mesh', 'edges shared by != 2 faces', '0 is ideal'),\n",
    "    ('self_intersection_flag', 1, 'Self-intersection proxy flag', 'boolean', '-', 'mesh', 'len(broken_faces) > 0', 'True/False or NaN'),\n",
    "    ('normal_consistency_flag', 1, 'Normal winding consistency', 'boolean', '-', 'mesh', 'mesh.is_winding_consistent', 'True/False'),\n",
    "    ('edge_length_mean', 1, 'Mean edge length', 'float', 'mm', 'mesh', 'mean(edges_unique_length)', 'varies'),\n",
    "    ('edge_length_std', 1, 'Edge length standard deviation', 'float', 'mm', 'mesh', 'std(edges_unique_length)', 'varies'),\n",
    "    ('triangle_aspect_ratio_max', 1, 'Maximum triangle aspect ratio', 'float', '-', 'mesh', 'max(longest_edge/shortest_edge)', '< 20 is good'),\n",
    "    ('layer1_pass', 1, 'Layer 1 gate pass status', 'boolean', '-', 'derived', 'PASS if at most 1 issue (lenient for AI meshes)', 'True/False'),\n",
    "    ('layer1_fail_reason', 1, 'Reason for Layer 1 failure', 'string', '-', 'derived', 'Concatenated failure reasons', 'text or empty'),\n",
    "    \n",
    "    # Layer 2\n",
    "    ('symmetry_axis_confidence', 2, 'Rotational symmetry confidence', 'float', '-', 'mesh', 'PCA max eigenvalue / sum', '0-1, higher=better'),\n",
    "    ('hub_plane_flatness_proxy', 2, 'Hub plane flatness RMSE', 'float', 'mm', 'mesh', 'RMSE of plane fit to hub region', '< 5mm is good'),\n",
    "    ('center_bore_roundness_proxy', 2, 'Center bore roundness residual', 'float', 'mm', 'mesh', 'Circle fit residual at hub', '< 3mm is good'),\n",
    "    ('bolt_hole_detectability_score', 2, 'Bolt hole detection confidence', 'float', '-', 'mesh', 'Not computed (NaN)', 'NaN'),\n",
    "    ('layer2_status', 2, 'Layer 2 gate status', 'string', '-', 'derived', 'PASS/FAIL/UNKNOWN based on thresholds', 'PASS/FAIL/UNKNOWN'),\n",
    "    ('layer2_fail_reason', 2, 'Reason for Layer 2 status', 'string', '-', 'derived', 'Concatenated issues', 'text or empty'),\n",
    "    \n",
    "    # Layer 3\n",
    "    ('modal_separation', 3, 'Mode 11 - Mode 7 frequency gap', 'float', 'Hz', 'derived', 'mode11_freq - mode7_freq', '600-1000 Hz typical'),\n",
    "    ('frequency_ratio', 3, 'Mode 11 / Mode 7 frequency ratio', 'float', '-', 'derived', 'mode11_freq / mode7_freq', '> 2.5 is good'),\n",
    "    ('stiffness_proxy_k7', 3, 'Stiffness proxy using Mode 7', 'float', 'N/m (proxy)', 'derived', 'mass * (2*pi*mode7_freq)^2', 'varies'),\n",
    "    ('stiffness_proxy_k11', 3, 'Stiffness proxy using Mode 11', 'float', 'N/m (proxy)', 'derived', 'mass * (2*pi*mode11_freq)^2', 'varies'),\n",
    "    ('nvh_margin', 3, 'NVH margin above 250 Hz', 'float', 'Hz', 'derived', 'mode7_freq - 250', '> 0 is good'),\n",
    "    ('layer3_mass_ok', 3, 'Mass within limit', 'boolean', '-', 'derived', 'mass <= 25 kg', 'True/False'),\n",
    "    ('layer3_mode7_ok', 3, 'Mode7 above threshold', 'boolean', '-', 'derived', 'mode7_freq >= 250 Hz', 'True/False'),\n",
    "    ('layer3_ratio_ok', 3, 'Frequency ratio above minimum', 'boolean', '-', 'derived', 'frequency_ratio >= 2.5', 'True/False'),\n",
    "    ('layer3_pass', 3, 'Layer 3 gate pass status', 'boolean', '-', 'derived', 'all Layer 3 checks pass', 'True/False'),\n",
    "    ('layer3_fail_reason', 3, 'Reason for Layer 3 failure', 'string', '-', 'derived', 'Concatenated failure reasons', 'text or empty'),\n",
    "    \n",
    "    # Layer 4\n",
    "    ('mass_ok', 4, 'Mass constraint satisfied', 'boolean', '-', 'derived', 'mass <= 25 kg', 'True/False'),\n",
    "    ('mode7_ok', 4, 'Mode7 constraint satisfied', 'boolean', '-', 'derived', 'mode7_freq >= 250 Hz', 'True/False'),\n",
    "    ('ratio_ok', 4, 'Ratio constraint satisfied', 'boolean', '-', 'derived', 'frequency_ratio >= 2.5', 'True/False'),\n",
    "    ('stiffness_ok', 4, 'Stiffness constraint satisfied', 'boolean', '-', 'derived', 'stiffness_proxy_k7 >= 10th percentile', 'True/False'),\n",
    "    ('constraint_violation_count', 4, 'Number of violated constraints', 'int', '-', 'derived', 'count of False in [mass_ok, mode7_ok, ratio_ok, stiffness_ok]', '0-4'),\n",
    "    ('passes_all_constraints', 4, 'All constraints satisfied', 'boolean', '-', 'derived', 'all constraint flags True', 'True/False'),\n",
    "    ('layer4_pass', 4, 'Layer 4 gate pass status', 'boolean', '-', 'derived', 'same as passes_all_constraints', 'True/False'),\n",
    "    ('layer4_fail_reason', 4, 'Reason for Layer 4 failure', 'string', '-', 'derived', 'List of violated constraints', 'text or empty'),\n",
    "    \n",
    "    # Layer 5\n",
    "    ('min_wall_thickness_est', 5, 'Estimated minimum wall thickness', 'float', 'mm', 'mesh', 'KDTree opposite surface distance', 'varies, > 3mm typical'),\n",
    "    ('sharp_edge_density', 5, 'Sharp edge density', 'float', 'edges/mm^2', 'mesh', 'edges with dihedral > 150 deg / area', 'lower is better'),\n",
    "    ('fillet_feasibility_score', 5, 'Fillet feasibility score', 'float', '-', 'derived', '1 - normalized(sharp_edge_density)', '0-1, higher is better'),\n",
    "    ('undercut_risk_score', 5, 'Undercut risk score', 'float', '-', 'mesh', 'Not computed (NaN)', 'NaN'),\n",
    "    ('balance_uniformity', 5, 'Radial balance uniformity', 'float', '-', 'mesh', 'CV of radial distances from axis', 'lower is better'),\n",
    "    ('cad_readiness_score', 5, 'CAD readiness composite score', 'float', '-', 'derived', '0.25*mesh_valid + 0.35*feat_extract + 0.40*manuf (nuanced)', '0-1, higher is better'),\n",
    "    ('layer5_status', 5, 'Layer 5 gate status', 'string', '-', 'derived', 'PASS/FAIL/UNKNOWN based on cad_readiness_score', 'PASS/FAIL/UNKNOWN'),\n",
    "    ('layer5_reason', 5, 'Reason for Layer 5 status', 'string', '-', 'derived', 'Explanation of status', 'text or empty'),\n",
    "    \n",
    "    # Layer 6\n",
    "    ('normalized_mass', 6, 'Min-max normalized mass', 'float', '-', 'derived', '(mass - min) / (max - min)', '0-1'),\n",
    "    ('normalized_mode7', 6, 'Min-max normalized Mode7', 'float', '-', 'derived', '(mode7 - min) / (max - min)', '0-1'),\n",
    "    ('z_mass', 6, 'Z-score normalized mass', 'float', '-', 'derived', '(mass - mean) / std', 'typically -3 to 3'),\n",
    "    ('z_mode7', 6, 'Z-score normalized Mode7', 'float', '-', 'derived', '(mode7 - mean) / std', 'typically -3 to 3'),\n",
    "    ('z_mode11', 6, 'Z-score normalized Mode11', 'float', '-', 'derived', '(mode11 - mean) / std', 'typically -3 to 3'),\n",
    "    ('z_frequency_ratio', 6, 'Z-score normalized frequency ratio', 'float', '-', 'derived', '(ratio - mean) / std', 'typically -3 to 3'),\n",
    "    ('mass_margin_norm', 6, 'Normalized mass margin', 'float', '-', 'derived', '(25 - mass) / 25', '> 0 is good'),\n",
    "    ('mode7_margin_norm', 6, 'Normalized Mode7 margin', 'float', '-', 'derived', '(mode7 - 250) / 250', '> 0 is good'),\n",
    "    ('ratio_margin_norm', 6, 'Normalized ratio margin', 'float', '-', 'derived', '(ratio - 2.5) / 2.5', '> 0 is good'),\n",
    "    ('stiffness_margin_norm', 6, 'Normalized stiffness margin', 'float', '-', 'derived', '(k7 - threshold) / threshold', '> 0 is good'),\n",
    "    ('design_margin_score', 6, 'Overall design margin', 'float', '-', 'derived', 'min of all normalized margins', 'higher is safer'),\n",
    "    ('risk_level', 6, 'Risk classification', 'string', '-', 'derived', 'High/Medium/Low based on percentile thresholds (P33/P67)', 'High/Medium/Low'),\n",
    "    ('vehicle_class_label', 6, 'Vehicle class classification', 'string', '-', 'derived', 'Not computed (UNKNOWN)', 'UNKNOWN'),\n",
    "]\n",
    "\n",
    "# Create data dictionary dataframe\n",
    "data_dict_df = pd.DataFrame(data_dict_entries, columns=[\n",
    "    'column_name', 'layer', 'description', 'data_type', 'unit', 'source', 'formula_or_logic', 'expected_range_or_values'\n",
    "])\n",
    "\n",
    "# Save to CSV\n",
    "data_dict_df.to_csv(OUTPUT_DATA_DICT_CSV, index=False)\n",
    "print(f\"Saved data dictionary to: {OUTPUT_DATA_DICT_CSV}\")\n",
    "print(f\"  Total entries: {len(data_dict_df)}\")\n",
    "\n",
    "# Show summary by layer\n",
    "print(f\"\\nEntries by layer:\")\n",
    "# Convert layer to string for sorting (handles mixed 'raw' and numeric values)\n",
    "layer_counts = data_dict_df['layer'].astype(str).value_counts()\n",
    "# Sort with 'raw' first, then numeric layers\n",
    "layer_order = ['raw'] + [str(i) for i in range(7)]\n",
    "for layer in layer_order:\n",
    "    if layer in layer_counts.index:\n",
    "        print(f\"  Layer {layer}: {layer_counts[layer]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating quality report...\n",
      "============================================================\n",
      "Saved quality report to: C:\\Users\\mahil.kr\\GL\\data-analytics-capstone\\data\\processed\\quality_report.json\n",
      "\n",
      "Quality Report Summary:\n",
      "  Total designs: 904\n",
      "  With STL: 904\n",
      "  With STEP: 904\n",
      "\n",
      "Layer Pass Rates:\n",
      "  layer0: 100.0% pass, 0.0% fail\n",
      "  layer1: 5.5% pass, 94.5% fail\n",
      "  layer2: 39.2% pass, 60.8% fail\n",
      "  layer3: 86.7% pass, 13.3% fail\n",
      "  layer4: 78.0% pass, 22.0% fail\n",
      "  layer5: 97.8% pass, 0.0% fail\n",
      "\n",
      "Top 5 Failure Reasons:\n",
      "  1. not_watertight: 854 occurrences\n",
      "  2. low_symmetry(0.44<0.45): 218 occurrences\n",
      "  3. aspect_ratio(39.3>39.3): 164 occurrences\n",
      "  4. hub_not_flat(6.4>6.4mm): 131 occurrences\n",
      "  5. hub_not_flat(6.5>6.4mm): 95 occurrences\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 15: Quality Report JSON\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Generating quality report...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build quality report\n",
    "quality_report = {\n",
    "    'summary': {\n",
    "        'total_designs': len(df_final),\n",
    "        'designs_with_stl': int(df_final['has_stl'].sum()),\n",
    "        'designs_with_step': int(df_final['has_step'].sum()),\n",
    "        'designs_missing_stl': int((~df_final['has_stl']).sum()),\n",
    "        'designs_missing_step': int((~df_final['has_step']).sum()),\n",
    "    },\n",
    "    'missing_rates': {},\n",
    "    'layer_failure_rates': {},\n",
    "    'top_failure_reasons': []\n",
    "}\n",
    "\n",
    "# Calculate missing rates for critical columns\n",
    "critical_columns = [\n",
    "    'bbox_x', 'bbox_y', 'bbox_z',\n",
    "    'triangle_count', 'is_watertight',\n",
    "    'symmetry_axis_confidence', 'hub_plane_flatness_proxy',\n",
    "    'modal_separation', 'frequency_ratio',\n",
    "    'cad_readiness_score', 'design_margin_score'\n",
    "]\n",
    "\n",
    "for col in critical_columns:\n",
    "    if col in df_final.columns:\n",
    "        missing_count = df_final[col].isna().sum()\n",
    "        missing_pct = 100 * missing_count / len(df_final)\n",
    "        quality_report['missing_rates'][col] = {\n",
    "            'count': int(missing_count),\n",
    "            'percent': round(missing_pct, 2)\n",
    "        }\n",
    "\n",
    "# Calculate failure rates per layer\n",
    "layer_gates = {\n",
    "    'layer0': 'layer0_pass',\n",
    "    'layer1': 'layer1_pass',\n",
    "    'layer2': 'layer2_status',\n",
    "    'layer3': 'layer3_pass',\n",
    "    'layer4': 'layer4_pass',\n",
    "    'layer5': 'layer5_status',\n",
    "}\n",
    "\n",
    "for layer_name, gate_col in layer_gates.items():\n",
    "    if gate_col in df_final.columns:\n",
    "        if df_final[gate_col].dtype == bool:\n",
    "            pass_count = df_final[gate_col].sum()\n",
    "            fail_count = len(df_final) - pass_count\n",
    "        else:\n",
    "            # String status column\n",
    "            pass_count = (df_final[gate_col] == 'PASS').sum()\n",
    "            fail_count = (df_final[gate_col] == 'FAIL').sum()\n",
    "            unknown_count = (df_final[gate_col] == 'UNKNOWN').sum()\n",
    "        \n",
    "        quality_report['layer_failure_rates'][layer_name] = {\n",
    "            'pass_count': int(pass_count),\n",
    "            'pass_percent': round(100 * pass_count / len(df_final), 2),\n",
    "            'fail_count': int(fail_count),\n",
    "            'fail_percent': round(100 * fail_count / len(df_final), 2)\n",
    "        }\n",
    "        \n",
    "        if df_final[gate_col].dtype != bool:\n",
    "            quality_report['layer_failure_rates'][layer_name]['unknown_count'] = int(unknown_count)\n",
    "            quality_report['layer_failure_rates'][layer_name]['unknown_percent'] = round(100 * unknown_count / len(df_final), 2)\n",
    "\n",
    "# Collect all failure reasons\n",
    "fail_reason_cols = [\n",
    "    'layer0_fail_reason', 'layer1_fail_reason', 'layer2_fail_reason',\n",
    "    'layer3_fail_reason', 'layer4_fail_reason', 'layer5_reason'\n",
    "]\n",
    "\n",
    "all_reasons = []\n",
    "for col in fail_reason_cols:\n",
    "    if col in df_final.columns:\n",
    "        reasons = df_final[col].dropna()\n",
    "        for reason_str in reasons:\n",
    "            if reason_str and reason_str.strip():\n",
    "                # Split compound reasons\n",
    "                for r in reason_str.split('; '):\n",
    "                    if r.strip():\n",
    "                        all_reasons.append(r.strip())\n",
    "\n",
    "# Count and get top 10\n",
    "reason_counts = Counter(all_reasons)\n",
    "top_reasons = reason_counts.most_common(10)\n",
    "quality_report['top_failure_reasons'] = [\n",
    "    {'reason': reason, 'count': count}\n",
    "    for reason, count in top_reasons\n",
    "]\n",
    "\n",
    "# Save to JSON\n",
    "with open(OUTPUT_QUALITY_JSON, 'w') as f:\n",
    "    json.dump(quality_report, f, indent=2)\n",
    "\n",
    "print(f\"Saved quality report to: {OUTPUT_QUALITY_JSON}\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nQuality Report Summary:\")\n",
    "print(f\"  Total designs: {quality_report['summary']['total_designs']}\")\n",
    "print(f\"  With STL: {quality_report['summary']['designs_with_stl']}\")\n",
    "print(f\"  With STEP: {quality_report['summary']['designs_with_step']}\")\n",
    "\n",
    "print(f\"\\nLayer Pass Rates:\")\n",
    "for layer, stats in quality_report['layer_failure_rates'].items():\n",
    "    print(f\"  {layer}: {stats['pass_percent']:.1f}% pass, {stats['fail_percent']:.1f}% fail\")\n",
    "\n",
    "print(f\"\\nTop 5 Failure Reasons:\")\n",
    "for i, item in enumerate(quality_report['top_failure_reasons'][:5], 1):\n",
    "    print(f\"  {i}. {item['reason']}: {item['count']} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating figures for synopsis...\n",
      "============================================================\n",
      "\n",
      "Creating Figure 1: Dataset preview...\n",
      "  Saved: C:\\Users\\mahil.kr\\GL\\data-analytics-capstone\\docs\\figures\\Figure1_dataset_preview.png\n",
      "\n",
      "Creating Figure 2: Folder tree structure...\n",
      "  Saved: C:\\Users\\mahil.kr\\GL\\data-analytics-capstone\\docs\\figures\\Figure2_folder_tree_structure.png\n",
      "\n",
      "Creating Figure 3: 3D wheel preview...\n",
      "  Saved: C:\\Users\\mahil.kr\\GL\\data-analytics-capstone\\docs\\figures\\Figure3_3d_wheel_preview.png\n",
      "\n",
      "All figures generated successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 16: Figures for the Synopsis\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Generating figures for synopsis...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ----- Figure 1: Dataset Preview (Table of first 10 rows) -----\n",
    "print(\"\\nCreating Figure 1: Dataset preview...\")\n",
    "\n",
    "# Select key columns for preview\n",
    "preview_cols = [\n",
    "    'file_name', 'mass', 'mode7_freq', 'mode11_freq',\n",
    "    'has_stl', 'layer0_pass', 'layer1_pass', 'layer4_pass',\n",
    "    'risk_level', 'design_margin_score'\n",
    "]\n",
    "preview_data = df_final[preview_cols].head(10).copy()\n",
    "\n",
    "# Format numeric columns\n",
    "preview_data['mass'] = preview_data['mass'].apply(lambda x: f\"{x:.2f}\")\n",
    "preview_data['mode7_freq'] = preview_data['mode7_freq'].apply(lambda x: f\"{x:.1f}\")\n",
    "preview_data['mode11_freq'] = preview_data['mode11_freq'].apply(lambda x: f\"{x:.1f}\")\n",
    "preview_data['design_margin_score'] = preview_data['design_margin_score'].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else 'NaN')\n",
    "\n",
    "# Create figure\n",
    "fig1, ax1 = plt.subplots(figsize=(16, 5))\n",
    "ax1.axis('off')\n",
    "ax1.set_title('DeepWheel Features Dataset - First 10 Rows Preview', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Create table\n",
    "table = ax1.table(\n",
    "    cellText=preview_data.values,\n",
    "    colLabels=preview_data.columns,\n",
    "    cellLoc='center',\n",
    "    loc='center',\n",
    "    colWidths=[0.12, 0.06, 0.08, 0.08, 0.06, 0.08, 0.08, 0.08, 0.08, 0.10]\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(8)\n",
    "table.scale(1.2, 1.5)\n",
    "\n",
    "# Style header\n",
    "for i in range(len(preview_cols)):\n",
    "    table[(0, i)].set_facecolor('#4472C4')\n",
    "    table[(0, i)].set_text_props(color='white', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG1_PREVIEW, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.close()\n",
    "print(f\"  Saved: {FIG1_PREVIEW}\")\n",
    "\n",
    "# ----- Figure 2: Folder Tree Structure -----\n",
    "print(\"\\nCreating Figure 2: Folder tree structure...\")\n",
    "\n",
    "tree_text = \"\"\"\n",
    "data-analytics-capstone/\n",
    "├── data/\n",
    "│   ├── deepwheel_sim_results.csv    [905 designs]\n",
    "│   ├── stl/                         [~850 STL mesh files]\n",
    "│   ├── step/                        [~904 STEP CAD files]\n",
    "│   ├── depth/                       [6249 depth images]\n",
    "│   ├── rgb/                         [6249 RGB images]\n",
    "│   └── processed/\n",
    "│       ├── deepwheel_features_full.csv\n",
    "│       ├── deepwheel_data_dictionary.csv\n",
    "│       └── quality_report.json\n",
    "├── docs/\n",
    "│   └── figures/\n",
    "│       ├── Figure1_dataset_preview.png\n",
    "│       ├── Figure2_folder_tree_structure.png\n",
    "│       └── Figure3_3d_wheel_preview.png\n",
    "├── notebooks/\n",
    "│   └── 00_preprocessing_and_feature_build.ipynb\n",
    "├── QM640_Final_Synopsis_Mahil_KR.docx\n",
    "└── README.md\n",
    "\"\"\"\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 8))\n",
    "ax2.axis('off')\n",
    "ax2.text(0.05, 0.95, 'Repository Structure', fontsize=14, fontweight='bold', \n",
    "         transform=ax2.transAxes, verticalalignment='top')\n",
    "ax2.text(0.05, 0.88, tree_text, fontsize=10, family='monospace',\n",
    "         transform=ax2.transAxes, verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG2_TREE, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "plt.close()\n",
    "print(f\"  Saved: {FIG2_TREE}\")\n",
    "\n",
    "# ----- Figure 3: 3D Wheel Preview -----\n",
    "print(\"\\nCreating Figure 3: 3D wheel preview...\")\n",
    "\n",
    "# Get sample STL files\n",
    "sample_stls = df_final[df_final['has_stl']]['stl_path'].head(4).tolist()\n",
    "\n",
    "if len(sample_stls) >= 1:\n",
    "    try:\n",
    "        n_samples = min(4, len(sample_stls))\n",
    "        fig3, axes = plt.subplots(1, n_samples, figsize=(4*n_samples, 4), \n",
    "                                   subplot_kw={'projection': '3d'})\n",
    "        if n_samples == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, (ax, stl_path) in enumerate(zip(axes, sample_stls[:n_samples])):\n",
    "            try:\n",
    "                mesh = trimesh.load(stl_path)\n",
    "                \n",
    "                # Get vertices and faces\n",
    "                vertices = mesh.vertices\n",
    "                faces = mesh.faces\n",
    "                \n",
    "                # Sample faces for faster rendering\n",
    "                if len(faces) > 5000:\n",
    "                    face_indices = np.random.choice(len(faces), 5000, replace=False)\n",
    "                    faces = faces[face_indices]\n",
    "                \n",
    "                # Create polygon collection\n",
    "                poly3d = [[vertices[idx] for idx in face] for face in faces]\n",
    "                collection = Poly3DCollection(poly3d, alpha=0.7, edgecolor='k', linewidths=0.1)\n",
    "                collection.set_facecolor('#4472C4')\n",
    "                ax.add_collection3d(collection)\n",
    "                \n",
    "                # Set axis limits\n",
    "                scale = vertices.max() - vertices.min()\n",
    "                center = vertices.mean(axis=0)\n",
    "                ax.set_xlim(center[0] - scale/2, center[0] + scale/2)\n",
    "                ax.set_ylim(center[1] - scale/2, center[1] + scale/2)\n",
    "                ax.set_zlim(center[2] - scale/2, center[2] + scale/2)\n",
    "                \n",
    "                # Get file name\n",
    "                file_name = Path(stl_path).stem\n",
    "                ax.set_title(f'{file_name[:20]}...', fontsize=9)\n",
    "                ax.set_xlabel('X')\n",
    "                ax.set_ylabel('Y')\n",
    "                ax.set_zlabel('Z')\n",
    "                \n",
    "            except Exception as e:\n",
    "                ax.text(0.5, 0.5, 0.5, f'Error loading\\n{str(e)[:30]}', \n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "        \n",
    "        plt.suptitle('Sample 3D Wheel Meshes from DeepWheel Dataset', fontsize=12, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIG3_WHEEL, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "        plt.close()\n",
    "        print(f\"  Saved: {FIG3_WHEEL}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not render 3D preview: {e}\")\n",
    "        print(\"  Creating placeholder figure...\")\n",
    "        \n",
    "        # Create placeholder\n",
    "        fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
    "        ax3.axis('off')\n",
    "        ax3.text(0.5, 0.5, '3D Wheel Preview\\n\\n(Rendering not available in this environment)\\n\\n'\n",
    "                 f'Dataset contains {df_final[\"has_stl\"].sum()} STL files',\n",
    "                 ha='center', va='center', fontsize=14, transform=ax3.transAxes)\n",
    "        plt.savefig(FIG3_WHEEL, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "        plt.close()\n",
    "        print(f\"  Saved placeholder: {FIG3_WHEEL}\")\n",
    "else:\n",
    "    print(\"  Warning: No STL files available for preview\")\n",
    "\n",
    "print(\"\\nAll figures generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to Commit + How to Reference in Synopsis\n",
    "\n",
    "---\n",
    "\n",
    "## Commit Checklist\n",
    "\n",
    "### Files to Commit\n",
    "\n",
    "1. **Notebook**\n",
    "   - `notebooks/00_preprocessing_and_feature_build.ipynb`\n",
    "\n",
    "2. **Generated Data Files**\n",
    "   - `data/processed/deepwheel_features_full.csv`\n",
    "   - `data/processed/deepwheel_data_dictionary.csv`\n",
    "   - `data/processed/quality_report.json`\n",
    "\n",
    "3. **Figures**\n",
    "   - `docs/figures/Figure1_dataset_preview.png`\n",
    "   - `docs/figures/Figure2_folder_tree_structure.png`\n",
    "   - `docs/figures/Figure3_3d_wheel_preview.png`\n",
    "\n",
    "### Git Commands\n",
    "\n",
    "```bash\n",
    "git add notebooks/00_preprocessing_and_feature_build.ipynb\n",
    "git add data/processed/\n",
    "git add docs/figures/\n",
    "git commit -m \"Add preprocessing notebook with 7-layer feature engineering\"\n",
    "git push origin main\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Synopsis References\n",
    "\n",
    "### GitHub Link\n",
    "\n",
    "Add to your synopsis:\n",
    "> The complete code and dataset are available at: https://github.com/[your-username]/data-analytics-capstone\n",
    "\n",
    "### Data Dictionary Reference\n",
    "\n",
    "> See **Appendix A: Data Dictionary** for complete column definitions. The full data dictionary is also available in the repository at `data/processed/deepwheel_data_dictionary.csv`.\n",
    "\n",
    "### Figure References\n",
    "\n",
    "- **Figure 1**: Dataset preview showing first 10 rows with key features and gate outputs\n",
    "- **Figure 2**: Repository folder structure showing data organization\n",
    "- **Figure 3**: Sample 3D wheel mesh renderings from the DeepWheel dataset\n",
    "\n",
    "---\n",
    "\n",
    "## Quality Summary for Synopsis\n",
    "\n",
    "Include these statistics in your methodology section:\n",
    "\n",
    "- **Total designs processed**: 905\n",
    "- **Feature columns**: 60+ (across 7 validation layers)\n",
    "- **Engineering gate layers**: 7 (Data Integrity, Mesh Validity, Feature Extractability, Physics Plausibility, Engineering Constraints, Manufacturability, ML Readiness)\n",
    "\n",
    "---\n",
    "\n",
    "## Important Disclaimers to Include\n",
    "\n",
    "1. Manufacturability metrics are **heuristic indicators**, not certified manufacturing approval\n",
    "2. Feature extractability scores are **confidence measures**, not guarantees\n",
    "3. All thresholds are **conservative and documented** based on typical automotive specifications\n",
    "4. Physics plausibility uses simulation outputs that should be **validated with physical testing** for production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
